import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
from sklearn.feature_extraction.text import TfidfVectorizer
import matplotlib.pyplot as plt
from itertools import combinations

def load_data():
    file_path = "/Users/jiyeonjoo/Desktop/ìµœì¢… í”Œì /ì—°ê´€í‚¤ì›Œë“œ 20250311 1634.xlsx"
    df = pd.read_excel(file_path, sheet_name='sheet')
    df = df.rename(columns={
        'ì—°ê´€í‚¤ì›Œë“œ': 'Keyword',
        'ì›”ê°„ê²€ìƒ‰ìˆ˜(PC)': 'Search_PC',
        'ì›”ê°„ê²€ìƒ‰ìˆ˜(ëª¨ë°”ì¼)': 'Search_Mobile',
        'ì›”í‰ê· í´ë¦­ìˆ˜(PC)': 'Click_PC',
        'ì›”í‰ê· í´ë¦­ìˆ˜(ëª¨ë°”ì¼)': 'Click_Mobile',
        'ì›”í‰ê· í´ë¦­ë¥ (PC)': 'CTR_PC',
        'ì›”í‰ê· í´ë¦­ë¥ (ëª¨ë°”ì¼)': 'CTR_Mobile',
        'ê²½ìŸì •ë„': 'Competition',
        'ì›”í‰ê· ë…¸ì¶œ ê´‘ê³ ìˆ˜': 'Ad_Impressions'
    })
    
    # ìˆ«ì ì»¬ëŸ¼ë“¤ ì •ë¦¬
    # ì½¤ë§ˆ(,) ì œê±° ë° float ë³€í™˜
    for col in ['Search_PC', 'Search_Mobile', 'Click_PC', 'Click_Mobile']:
        df[col] = df[col].astype(str).str.replace(',', '').astype(float)
    
    # % ê¸°í˜¸ ì œê±° ë° float ë³€í™˜
    for col in ['CTR_PC', 'CTR_Mobile']:
        df[col] = df[col].astype(str).str.replace('%', '').astype(float) / 100
    
    # ì´ ê²€ìƒ‰ëŸ‰ ê³„ì‚°
    df['Total_Search'] = df['Search_PC'] + df['Search_Mobile']
    
    return df

def identify_core_keywords(df):
    st.subheader("ğŸ” í•µì‹¬ í‚¤ì›Œë“œ ì‹ë³„ & í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„")
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“Œ í•µì‹¬ í‚¤ì›Œë“œ")
        core_keywords = df.nlargest(10, 'Total_Search')[['Keyword', 'Total_Search']]
        st.dataframe(core_keywords)
    
    with col2:
        st.subheader("ğŸ“Œ í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§")
        df_cluster = df[['Total_Search', 'Click_PC', 'Click_Mobile']].fillna(0)
        scaler = StandardScaler()
        df_scaled = scaler.fit_transform(df_cluster)
        
        kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)
        df['Cluster'] = kmeans.fit_predict(df_scaled)
        
        fig = px.scatter(df, x='Total_Search', y='Click_Mobile', color=df['Cluster'].astype(str),
                         hover_data=['Keyword'], title='í‚¤ì›Œë“œ í´ëŸ¬ìŠ¤í„°ë§')
        st.plotly_chart(fig)
    
    st.subheader("ğŸ“Š í´ëŸ¬ìŠ¤í„°ë³„ í‚¤ì›Œë“œ ë¶„ì„")
    cluster_groups = df.groupby('Cluster')
    cols = st.columns(2)
    
    for i, (cluster, group) in enumerate(cluster_groups):
        with cols[i % 2]:
            st.write(f"### Cluster {cluster} ëŒ€í‘œ í‚¤ì›Œë“œ")
            st.dataframe(group[['Keyword', 'Total_Search']].nlargest(3, 'Total_Search'))

def sns_keywords_and_item_dashboard(df):
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ“² SNS í¬ë¡¤ë§ì„ ìœ„í•œ í‚¤ì›Œë“œ")
        sns_keywords = df[(df['Total_Search'] > df['Total_Search'].median()) & (df['Click_Mobile'] > df['Click_Mobile'].median())]
        sns_keywords['Hashtag'] = sns_keywords['Keyword'].apply(lambda x: f"#{x.replace(' ', '')}")
        st.dataframe(sns_keywords[['Keyword', 'Total_Search', 'Click_Mobile', 'Hashtag']].head(10))
    
    with col2:
        st.subheader("ğŸ“¦ ì•„ì´í…œ ë¶„ë¥˜ ë° ìˆœìœ„ ë¶„ì„")
        item_mapping = {
            'ìƒì˜': ["íƒ‘", "ì…”ì¸ ", "ë¸”ë¼ìš°ìŠ¤", "ë‹ˆíŠ¸ì›¨ì–´"],
            'í•˜ì˜': ["íŒ¬ì¸ ", "ì²­ë°”ì§€", "ìŠ¤ì»¤íŠ¸"],
            'ì•„ìš°í„°': ["ì¬í‚·", "ì½”íŠ¸", "íŒ¨ë”©", "ì í¼", "ë² ìŠ¤íŠ¸"],
            'ì›í”¼ìŠ¤': ["ë“œë ˆìŠ¤", "ì í”„ìˆ˜íŠ¸"],
            'ê¸°íƒ€': ["ìºì£¼ì–¼ìƒì˜"]
        }
        
        def map_item(keyword):
            for category, keywords in item_mapping.items():
                if any(k in keyword for k in keywords):
                    return category
            return "ê¸°íƒ€"
        
        df['Item_Category'] = df['Keyword'].apply(map_item)
        item_rank = df.groupby('Item_Category')['Total_Search'].sum().reset_index().sort_values(by='Total_Search', ascending=False)
        st.dataframe(item_rank)

def analyze_keyword_associations(df):
    """í‚¤ì›Œë“œ ê°„ ì—°ê´€ë„ ë¶„ì„ ë° ì‹œê°í™”"""
    st.subheader("ğŸ”„ í‚¤ì›Œë“œ ì—°ê´€ë„ ë¶„ì„")
    
    # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
    top_n = st.slider("ë¶„ì„í•  ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜", 5, 30, 15)
    top_keywords = df.nlargest(top_n, 'Total_Search')
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ë„ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±
    features = ['Total_Search', 'Click_PC', 'Click_Mobile']
    
    # CTR ì»¬ëŸ¼ì´ ì ì ˆí•˜ê²Œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if df['CTR_PC'].dtype == np.float64 and df['CTR_Mobile'].dtype == np.float64:
        features.extend(['CTR_PC', 'CTR_Mobile'])
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    keyword_features = top_keywords[features].fillna(0)
    # ì •ê·œí™”
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(keyword_features)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarity_matrix = cosine_similarity(scaled_features)
    
    # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
    G = nx.Graph()
    
    # ë…¸ë“œ ì¶”ê°€
    for idx, keyword in enumerate(top_keywords['Keyword']):
        G.add_node(keyword, size=float(top_keywords.iloc[idx]['Total_Search']))
    
    # ì—°ê´€ë„ ì„ê³„ê°’ (0.5 ì´ìƒì¸ ê²½ìš°ë§Œ ì—°ê²°ì„  í‘œì‹œ)
    threshold = st.slider("ì—°ê´€ë„ ì„ê³„ê°’", 0.0, 1.0, 0.5, 0.1)
    
    # ê°„ì„  ì¶”ê°€
    for i in range(len(top_keywords)):
        for j in range(i+1, len(top_keywords)):
            if similarity_matrix[i, j] > threshold:
                G.add_edge(
                    top_keywords.iloc[i]['Keyword'], 
                    top_keywords.iloc[j]['Keyword'],
                    weight=similarity_matrix[i, j]
                )
    
    # ì‹œê°í™”
    col1, col2 = st.columns([2, 1])
    
    with col1:
        # ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ ìƒì„±
        pos = nx.spring_layout(G, seed=42)
        
        # ë…¸ë“œì™€ ê°„ì„  ì •ë³´ ì¶”ì¶œ
        edge_x = []
        edge_y = []
        for edge in G.edges():
            x0, y0 = pos[edge[0]]
            x1, y1 = pos[edge[1]]
            edge_x.extend([x0, x1, None])
            edge_y.extend([y0, y1, None])
            
        node_x = []
        node_y = []
        node_text = []
        node_size = []
        for node in G.nodes():
            x, y = pos[node]
            node_x.append(x)
            node_y.append(y)
            node_text.append(node)
            # ê²€ìƒ‰ëŸ‰ì— ë”°ë¼ ë…¸ë“œ í¬ê¸° ì¡°ì •
            size = G.nodes[node]['size'] / top_keywords['Total_Search'].max() * 30
            node_size.append(size + 10)  # ìµœì†Œ í¬ê¸° 10ìœ¼ë¡œ ì„¤ì •
            
        # ê°„ì„  íŠ¸ë ˆì´ìŠ¤
        edge_trace = go.Scatter(
            x=edge_x, y=edge_y,
            line=dict(width=0.5, color='#888'),
            hoverinfo='none',
            mode='lines')
        
        # ë…¸ë“œ íŠ¸ë ˆì´ìŠ¤
        node_trace = go.Scatter(
            x=node_x, y=node_y,
            mode='markers+text',
            text=node_text,
            textposition="top center",
            hoverinfo='text',
            marker=dict(
                showscale=True,
                colorscale='YlGnBu',
                size=node_size,
                color=[G.degree(node) for node in G.nodes()],
                colorbar=dict(
                    thickness=15,
                    title='ì—°ê²° ê°•ë„',
                    xanchor='left',
                    titleside='right'
                ),
                line_width=2))
        
        # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
        fig = go.Figure(data=[edge_trace, node_trace],
                        layout=go.Layout(
                            title='í‚¤ì›Œë“œ ì—°ê´€ë„ ë„¤íŠ¸ì›Œí¬',
                            showlegend=False,
                            hovermode='closest',
                            margin=dict(b=20, l=5, r=5, t=40),
                            xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                            yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))
        
        st.plotly_chart(fig)
        
    with col2:
        st.subheader("ğŸ’¡ ì£¼ìš” ì—°ê´€ í‚¤ì›Œë“œ ìŒ")
        # ì—°ê´€ë„ê°€ ë†’ì€ í‚¤ì›Œë“œ ìŒ ì¶”ì¶œ
        keyword_pairs = []
        for i in range(len(top_keywords)):
            for j in range(i+1, len(top_keywords)):
                if similarity_matrix[i, j] > threshold:
                    keyword_pairs.append({
                        'í‚¤ì›Œë“œ 1': top_keywords.iloc[i]['Keyword'],
                        'í‚¤ì›Œë“œ 2': top_keywords.iloc[j]['Keyword'],
                        'ì—°ê´€ë„': similarity_matrix[i, j]
                    })
                    
        # ì—°ê´€ë„ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        if keyword_pairs:
            pairs_df = pd.DataFrame(keyword_pairs).sort_values('ì—°ê´€ë„', ascending=False)
            st.dataframe(pairs_df)
        else:
            st.info("ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë” ë§ì€ ì—°ê´€ í‚¤ì›Œë“œ ìŒì„ í™•ì¸í•˜ì„¸ìš”.")

def analyze_keyword_similarity(df):
    """í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„"""
    st.subheader("ğŸ”¤ í‚¤ì›Œë“œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„")
    
    # í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì¢‹ì§€ë§Œ, ë‹¨ìˆœ ë¶„ì„ìœ¼ë¡œ ì§„í–‰
    # í‚¤ì›Œë“œ ì „ì²˜ë¦¬
    keywords = df['Keyword'].tolist()
    
    # TF-IDF ë²¡í„°í™”
    vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 2))
    tfidf_matrix = vectorizer.fit_transform(keywords)
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
    
    # ì¸í„°ë™í‹°ë¸Œ ìœ ì‚¬ë„ ê²€ìƒ‰
    col1, col2 = st.columns([1, 1])
    
    with col1:
        selected_keyword = st.selectbox("í‚¤ì›Œë“œ ì„ íƒ", df['Keyword'].tolist())
        num_similar = st.slider("ìœ ì‚¬ í‚¤ì›Œë“œ ìˆ˜", 5, 20, 10)
        
        if selected_keyword:
            idx = df.index[df['Keyword'] == selected_keyword].tolist()[0]
            sim_scores = list(enumerate(cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:num_similar+1]  # ì²« ë²ˆì§¸ëŠ” ìê¸° ìì‹ ì´ë¯€ë¡œ ì œì™¸
            similar_indices = [i[0] for i in sim_scores]
            similar_keywords = df.iloc[similar_indices][['Keyword', 'Total_Search']]
            similar_keywords['ìœ ì‚¬ë„'] = [i[1] for i in sim_scores]
            
            st.dataframe(similar_keywords.sort_values('ìœ ì‚¬ë„', ascending=False))
    
    with col2:
        # ì›Œë“œ í´ë¼ìš°ë“œ ìƒì„±ì„ ìœ„í•œ ìœ ì‚¬ í‚¤ì›Œë“œë“¤ì˜ ë¹ˆë„ ê³„ì‚°
        st.subheader("ìœ ì‚¬ í‚¤ì›Œë“œ ì‹œê°í™”")
        
        # ìœ ì‚¬ë„ íˆíŠ¸ë§µ
        if len(df) > 30:  # ë°ì´í„°ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ íˆíŠ¸ë§µì´ ë³µì¡í•´ì§ˆ ìˆ˜ ìˆìŒ
            top_keys = df.nlargest(20, 'Total_Search')['Keyword'].tolist()
            mask = df['Keyword'].isin(top_keys)
            subset_df = df[mask].reset_index(drop=True)
            
            # ì„œë¸Œì…‹ì— ëŒ€í•œ TF-IDF ë° ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë‹¤ì‹œ ê³„ì‚°
            subset_keywords = subset_df['Keyword'].tolist()
            subset_tfidf = vectorizer.fit_transform(subset_keywords)
            subset_cosine_sim = cosine_similarity(subset_tfidf, subset_tfidf)
            
            # íˆíŠ¸ë§µ ê·¸ë¦¬ê¸°
            fig = px.imshow(
                subset_cosine_sim,
                labels=dict(x="í‚¤ì›Œë“œ", y="í‚¤ì›Œë“œ", color="ìœ ì‚¬ë„"),
                x=subset_keywords,
                y=subset_keywords,
                color_continuous_scale="Viridis"
            )
            fig.update_layout(
                title="ìƒìœ„ 20ê°œ í‚¤ì›Œë“œ ìœ ì‚¬ë„ íˆíŠ¸ë§µ",
                xaxis_tickangle=-45
            )
            st.plotly_chart(fig)
        else:
            st.info("ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

def keyword_association_analysis(df):
    """í‚¤ì›Œë“œ ì—°ê´€ê·œì¹™ ë¶„ì„ (ì•„ì´í…œ ê¸°ë°˜)"""
    st.subheader("ğŸ”— í‚¤ì›Œë“œ ì—°ê´€ê·œì¹™ ë¶„ì„")
    
    # í‚¤ì›Œë“œì—ì„œ ì£¼ìš” ë‹¨ì–´ ì¶”ì¶œ
    def extract_key_terms(keyword):
        # ë‹¨ìˆœ ê³µë°± ê¸°ì¤€ ë¶„ë¦¬ (í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ì •í™•í•¨)
        return keyword.split()
    
    # í‚¤ì›Œë“œì—ì„œ ì£¼ìš” ìš©ì–´ ì¶”ì¶œ
    df['terms'] = df['Keyword'].apply(extract_key_terms)
    
    # ìš©ì–´ ë¹ˆë„ ê³„ì‚°
    all_terms = []
    for terms in df['terms']:
        all_terms.extend(terms)
    
    term_freq = pd.Series(all_terms).value_counts().reset_index()
    term_freq.columns = ['Term', 'Frequency']
    
    # ë¹ˆë„ê°€ ë†’ì€ ìš©ì–´ ì¶”ì¶œ
    min_support = st.slider("ìµœì†Œ ì§€ì§€ë„ (ìµœì†Œ ì¶œí˜„ ë¹ˆë„)", 2, 10, 3)
    frequent_terms = term_freq[term_freq['Frequency'] >= min_support]['Term'].tolist()
    
    # ì—°ê´€ê·œì¹™ ë¶„ì„ì„ ìœ„í•œ ìš©ì–´ ìŒ ìƒì„±
    term_pairs = []
    support_dict = {}
    confidence_dict = {}
    
    # ì§€ì§€ë„ì™€ ì‹ ë¢°ë„ ê³„ì‚°
    for terms in df['terms']:
        # ë¹ˆë²ˆ ìš©ì–´ë§Œ í¬í•¨
        filtered_terms = [term for term in terms if term in frequent_terms]
        
        # ìš©ì–´ ìŒ ì¡°í•© ìƒì„±
        for pair in combinations(filtered_terms, 2):
            sorted_pair = tuple(sorted(pair))
            if sorted_pair in support_dict:
                support_dict[sorted_pair] += 1
            else:
                support_dict[sorted_pair] = 1
    
    # ì§€ì§€ë„ë¥¼ ë¹„ìœ¨ë¡œ ë³€í™˜
    total_records = len(df)
    for pair, count in support_dict.items():
        support_dict[pair] = count / total_records
    
    # ì‹ ë¢°ë„ ê³„ì‚° (A â†’ B)
    for pair in support_dict:
        term1, term2 = pair
        # term1ì´ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œ ìˆ˜
        term1_count = sum(1 for terms in df['terms'] if term1 in terms)
        # term2ê°€ ë‚˜íƒ€ë‚œ í‚¤ì›Œë“œ ìˆ˜
        term2_count = sum(1 for terms in df['terms'] if term2 in terms)
        
        # ì‹ ë¢°ë„: P(B|A) = support(A,B) / support(A)
        confidence_1_to_2 = support_dict[pair] * total_records / term1_count if term1_count > 0 else 0
        confidence_2_to_1 = support_dict[pair] * total_records / term2_count if term2_count > 0 else 0
        
        term_pairs.append({
            'ìš©ì–´1': term1,
            'ìš©ì–´2': term2,
            'ì§€ì§€ë„': support_dict[pair],
            'ì‹ ë¢°ë„(1â†’2)': confidence_1_to_2,
            'ì‹ ë¢°ë„(2â†’1)': confidence_2_to_1,
            'í–¥ìƒë„': (support_dict[pair] * total_records) / (term1_count * term2_count) if term1_count * term2_count > 0 else 0
        })
    
    # ê²°ê³¼ ì •ë ¬ ë° í‘œì‹œ
    if term_pairs:
        term_pairs_df = pd.DataFrame(term_pairs).sort_values('í–¥ìƒë„', ascending=False)
        st.dataframe(term_pairs_df)
        
        # ì‹œê°í™”
        fig = px.scatter(
            term_pairs_df.head(20),
            x='ì§€ì§€ë„',
            y='í–¥ìƒë„',
            size='ì‹ ë¢°ë„(1â†’2)',
            color='ì‹ ë¢°ë„(2â†’1)',
            hover_data=['ìš©ì–´1', 'ìš©ì–´2'],
            title='ìš©ì–´ ì—°ê´€ê·œì¹™ ë¶„ì„ (ìƒìœ„ 20ê°œ)'
        )
        st.plotly_chart(fig)
    else:
        st.info("ì¶©ë¶„í•œ ì—°ê´€ê·œì¹™ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìµœì†Œ ì§€ì§€ë„ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")

def main():
    st.title("ğŸ“Š í‚¤ì›Œë“œ ë¶„ì„ ëŒ€ì‹œë³´ë“œ")
    df = load_data()
    
    # ê¸°ì¡´ ë¶„ì„
    identify_core_keywords(df)
    sns_keywords_and_item_dashboard(df)
    
    # ìƒˆë¡œìš´ ì—°ê´€ë„ ë° ìœ ì‚¬ë„ ë¶„ì„
    st.title("ğŸ”„ í‚¤ì›Œë“œ ì—°ê´€ë„ ë° ìœ ì‚¬ë„ ë¶„ì„")
    
    # ì—°ê´€ë„ ë¶„ì„ íƒ­
    analyze_keyword_associations(df)
    
    # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ë¶„ì„ íƒ­
    analyze_keyword_similarity(df)
    
    # ì—°ê´€ê·œì¹™ ë¶„ì„ íƒ­
    keyword_association_analysis(df)

if __name__ == "__main__":
    main()