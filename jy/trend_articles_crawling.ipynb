{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 12개의 기사 링크 발견!\n",
      "✅ 크롤링 성공: 티셔츠에 청바지, 2025년엔 ‘이렇게’ 입으세요 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 올해 우리의 옷 입는 방식을 결정할 사진 3장 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 치마 잘 입고 싶다면 새겨두어야 할 요즘 조합 6 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 1990년대 감성의 이 패턴이 2025년 봄에 맞춰 돌아왔습니다 | 업로드 날짜: 2025.02.28\n",
      "✅ 크롤링 성공: 올봄 못 이기는 척 입어보고픈, 스키니 진 조합 4 | 업로드 날짜: 2025.02.27\n",
      "✅ 크롤링 성공: 2025년 봄, 성공적인 스타일링 완성할 블레이저 3 | 업로드 날짜: 2025.02.26\n",
      "✅ 크롤링 성공: 예쁜 상의의 기준이 될, 2025 민소매 톱 트렌드 | 업로드 날짜: 2025.02.26\n",
      "✅ 크롤링 성공: 반바지에 흰 운동화, 올봄에 입고 싶은 나긋한 룩! | 업로드 날짜: 2025.02.25\n",
      "✅ 크롤링 성공: 낮을수록 치명적인, 올봄 스커트 스타일링 | 업로드 날짜: 2025.02.25\n",
      "✅ 크롤링 성공: 발레코어 다음! 2025년을 정의할 ‘이 스타일’ | 업로드 날짜: 2025.02.24\n",
      "✅ 크롤링 성공: 올해 헐렁한 청바지에 꼭 신어봐야 할 신발 | 업로드 날짜: 2025.02.21\n",
      "✅ 크롤링 성공: 올 봄과 여름 내내 돌려 입게 될 손쉬운 원피스 7 | 업로드 날짜: 2025.02.21\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_vogue_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## 보그 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://www.vogue.co.kr/fashion/fashion-trend/\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 기사 링크 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 링크를 저장할 리스트\n",
    "article_links = []\n",
    "\n",
    "# 기사 링크가 포함된 <a> 태그 찾기\n",
    "for a_tag in soup.select(\"a[href^='/2025']\"):  # 2025년 기사만 가져오기\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # 상대 경로를 절대 경로로 변환\n",
    "    full_url = \"https://www.vogue.co.kr\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # 중복 제거\n",
    "        article_links.append(full_url)\n",
    "\n",
    "print(f\"🔗 총 {len(article_links)}개의 기사 링크 발견!\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 제목과 본문 크롤링\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 제목 추출\n",
    "            title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"제목 없음\"\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3️⃣ 본문에서 첫 번째 줄 삭제, 두 번째 줄을 업로드 날짜로 저장\n",
    "            lines = content.split(\"\\n\")  # 줄 단위로 분리\n",
    "            if len(lines) > 1:\n",
    "                upload_date = lines[1].strip()  # 두 번째 줄이 업로드 날짜\n",
    "                content = \"\\n\".join(lines[2:]).strip()  # 첫째 줄 & 둘째 줄 제거 후 본문 재구성\n",
    "            else:\n",
    "                upload_date = \"날짜 없음\"\n",
    "                content = \"\"  # 내용이 없을 경우 빈 문자열 처리\n",
    "\n",
    "            # 4️⃣ 특정 패턴 이후 모든 내용 삭제\n",
    "            pattern = r\"(패션 뉴스|패션 트렌드|패션 아이템|여행|셀러브리티 스타일|웰니스|뷰 포인트)\\n\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[가-힣]+.*\"\n",
    "            content = re.sub(pattern, \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"업로드 날짜\": upload_date,\n",
    "                \"제목\": title,\n",
    "                \"본문\": content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 업로드 날짜: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"업로드 날짜\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "csv_filename = f\"{scrape_date}_vogue_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 32개의 기사 링크 발견!\n",
      "✅ 크롤링 성공: 이번 봄에 준비해야 할 플라워 트렌드\n",
      "✅ 크롤링 성공: 2025 트렌드 그 자체가 될 뉴 백\n",
      "✅ 크롤링 성공: 막스마라, 재킷, 여자\n",
      "✅ 크롤링 성공: 지수가 등장한 루브르의 밤에선 무슨 일이?\n",
      "✅ 크롤링 성공: 허리에 두르는 벨트 백 아니고 벨트가 달려있는 벨트 백\n",
      "✅ 크롤링 성공: 핀터레스트를 앞선 사진가가 있다?\n",
      "✅ 크롤링 성공: 봄 패션 컬러는 무조건 연두, 연두, 연두!\n",
      "✅ 크롤링 성공: 데일리 스니커즈의 귀환\n",
      "✅ 크롤링 성공: 아카데미 시상식에 후디 입고 나타난 이 남자\n",
      "✅ 크롤링 성공: 아카데미 시상식에서 발견한 올해의 드레스 트렌드\n",
      "✅ 크롤링 성공: 21세기 공주처럼 입는 패션 트렌드\n",
      "✅ 크롤링 성공: 밀라노에서 발견한 올해를 지배할 스커트 트렌드\n",
      "✅ 크롤링 성공: 밤에도 빛나는 보테가 베네타의 새 컬렉션\n",
      "✅ 크롤링 성공: 패션 트렌드를 질주하는 바이커와 카레이서\n",
      "✅ 크롤링 성공: 가장 '나나' 같은 스타일\n",
      "✅ 크롤링 성공: 2025 트렌드 찾으려면 패션스쿨 학생들만 보세요\n",
      "✅ 크롤링 성공: 아이린과 디올의 강렬한 파동\n",
      "✅ 크롤링 성공: 런던 패션위크에서 발견한 올해의 패션 트렌드\n",
      "✅ 크롤링 성공: 셀린느와 함께한 봄의 어느 낮\n",
      "✅ 크롤링 성공: 2025년, 결국 패션 트렌드가 된 이 바지\n",
      "✅ 크롤링 성공: 디올의 강렬한 에너지가 담긴 백\n",
      "✅ 크롤링 성공: 억 소리 나는 시계로 또 한 번 레전드 갱신한 지드래곤\n",
      "✅ 크롤링 성공: 베르사체가 쏘아 올린 메탈 신드롬\n",
      "✅ 크롤링 성공: 페라가모와 발레 코어의 로맨틱한 앙상블\n",
      "✅ 크롤링 성공: 벨라 하디드의 비키니엔 뭔가 특별한 것이 있다\n",
      "✅ 크롤링 성공: 지드래곤과 퍼렐 윌리엄스가 한국에 초특급 행사를 연다\n",
      "✅ 크롤링 성공: 접속, 패션 무비 월드\n",
      "✅ 크롤링 성공: 주름과 볼륨, 올여름 실패 없는 스커트 공식\n",
      "✅ 크롤링 성공: 요즘 아이돌의 스타킹은 다르다\n",
      "✅ 크롤링 성공: 풀오버부터 카디건까지, 모델 곽지영의 '내돈내산' 니트 아이템\n",
      "✅ 크롤링 성공: 가을 스커트에 안 신어주면 서운한 이 슈즈\n",
      "✅ 크롤링 성공: 화이트 셔츠, 이렇게 입어요!\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025-03-07_elle_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## 엘르 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL (패션 트렌드 페이지)\n",
    "main_url = \"https://www.elle.co.kr/fashion/trends\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "# 1️⃣ 메인 페이지에서 기사 링크 및 제목 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트\n",
    "article_data = []\n",
    "\n",
    "# 기사 링크와 제목 크롤링\n",
    "for article in soup.select(\"div.atcbox\"):\n",
    "    # 링크 찾기\n",
    "    a_tag = article.select_one(\"a[href^='/article/']\")\n",
    "    if not a_tag:\n",
    "        continue\n",
    "    \n",
    "    article_url = \"https://www.elle.co.kr\" + a_tag[\"href\"]\n",
    "    \n",
    "    # 제목 찾기\n",
    "    title_tag = article.select_one(\"p.tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    article_data.append({\"제목\": title, \"기사 URL\": article_url})  # URL 포함\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견!\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문 크롤링\n",
    "for article in article_data:\n",
    "    title, url = article[\"제목\"], article[\"기사 URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\"스크래핑 날짜\": scrape_date, \"제목\": title, \"본문\": content, \"기사 URL\": url})\n",
    "            print(f\"✅ 크롤링 성공: {title}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "csv_filename = f\"{scrape_date}_elle_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 15개의 기사 링크 발견!\n",
      "✅ 크롤링 성공: 올봄 트렌치코트는 짧은 길이가 정답입니다 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 티모시를 위한 카일리 제너의 관능적인 드레스 3종 세트 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 부담 없이 도전하는 올 레드 룩 연출법 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 올해 웨지 슈즈가 유행이라는 소문은 진짜다 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: '꾸안꾸 마스터' 카이아 거버의 볼캡 활용법 | 업로드 날짜: 2025.03.06\n",
      "✅ 크롤링 성공: 가장 쉽게 세련된 방식으로 재킷을 입는 법 (제니 VER.) | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 올봄 가장 실속 있는 조합, 셔츠에 베스트 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 다리가 길어보이는 제니의 하의 실종 룩 공식 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 르세라핌 김채원, 더블유 디지털 커버 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 우아하고도 부드러운 올봄 재킷 컬러는? | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 그대로 따라 입고 싶은 모델 3인의 봄 아웃핏 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 로맨틱 vs 캐주얼, 산뜻한 봄나들이 룩을 위한 팬츠 2 | 업로드 날짜: 2025.03.05\n",
      "✅ 크롤링 성공: 올봄엔 '이것'만 매면 스타일리시해 져요 | 업로드 날짜: 2025.03.04\n",
      "✅ 크롤링 성공: 개인 디자이너 브랜드로 완성한 리사의 오스카 모먼트 | 업로드 날짜: 2025.03.04\n",
      "✅ 크롤링 성공: 제 97회 오스카 시상식을 빛낸 샤넬 | 업로드 날짜: 2025.03.04\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_wkorea_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## W코리아 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://www.wkorea.com/fashion/\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 기사 링크 및 제목 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트\n",
    "article_data = []\n",
    "\n",
    "# 기사 목록 크롤링 (`li` 태그 기준으로 탐색)\n",
    "for article in soup.select(\"li\"):\n",
    "    # 기사 링크 찾기 (`a` 태그 내부의 href 속성)\n",
    "    a_tag = article.select_one(\"a[href^='https://www.wkorea.com/2025']\")\n",
    "    if not a_tag:\n",
    "        continue  # 기사 링크가 없으면 무시\n",
    "\n",
    "    article_url = a_tag[\"href\"]\n",
    "\n",
    "    # 기사 제목 찾기 (`h3.s-tit` 태그 내부)\n",
    "    title_tag = article.select_one(\"h3.s_tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 기사 URL과 제목을 리스트에 추가\n",
    "    article_data.append({\"제목\": title, \"기사 URL\": article_url})\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견!\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문 크롤링 & 업로드 날짜 추출\n",
    "for article in article_data:\n",
    "    title, url = article[\"제목\"], article[\"기사 URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "             # 3️⃣ 본문 첫 줄 & 두 번째 줄에서 업로드 날짜 & 에디터 추출\n",
    "            lines = content.split(\"\\n\")  # 본문을 줄 단위로 분리\n",
    "            if len(lines) > 1:\n",
    "                # \"W Fashion YYYY.MM.DD\" 형식에서 \"W Fashion\" 제거 후 날짜만 저장\n",
    "                upload_date_match = re.match(r\"W Fashion\\s*(\\d{4}\\.\\d{2}\\.\\d{2})\", lines[0])\n",
    "                upload_date = upload_date_match.group(1) if upload_date_match else \"날짜 없음\"\n",
    "                \n",
    "                editor = lines[1].strip()  # 두 번째 줄 (에디터)\n",
    "            else:\n",
    "                upload_date = \"날짜 없음\"\n",
    "                editor = \"미상\"\n",
    "\n",
    "            # 4️⃣ 본문에서 첫 두 줄(업로드 날짜 & 에디터) 제거\n",
    "            content = \"\\n\".join(lines[2:]).strip() if len(lines) > 2 else \"\"\n",
    "\n",
    "            # 5️⃣ 본문 끝부분의 \"W Fashion YYYY.MM.DD by 이름\" 패턴 제거\n",
    "            content = re.sub(r\"W\\s*Fashion\\n*\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[가-힣]+.*\", \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"업로드 날짜\": upload_date,\n",
    "                \"에디터\": editor,\n",
    "                \"제목\": title,\n",
    "                \"본문\": content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 업로드 날짜: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 6️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"업로드 날짜\", \"에디터\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "csv_filename = f\"{scrape_date}_wkorea_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 32개의 기사 링크 발견!\n",
      "✅ 크롤링 성공: 밀라노에서 발견한 올해를 지배할 스커트 트렌드\n",
      "✅ 크롤링 성공: 구찌, 익숙한 것을 낯설게 만드는 힘\n",
      "✅ 크롤링 성공: 런던 패션위크에서 발견한 올해의 패션 트렌드\n",
      "✅ 크롤링 성공: 베르사체가 쏘아 올린 메탈 신드롬\n",
      "✅ 크롤링 성공: 제니, 고윤정도 반한 샤넬의 오트 쿠튀르 컬렉션\n",
      "✅ 크롤링 성공: 제이홉, 배두나가 등판한  파리 맨즈 패션위크 하이라이트\n",
      "✅ 크롤링 성공: 있지, 유나가 입어서  더 입고 싶은 이것?\n",
      "✅ 크롤링 성공: 여기가 뉴욕 패션위크야, 디즈니랜드야\n",
      "✅ 크롤링 성공: 뉴요커 중의 뉴요커는 누구?\n",
      "✅ 크롤링 성공: 롱코트 하나로 단숨에 뉴요커 되는 법\n",
      "✅ 크롤링 성공: 코펜하겐 힙스터처럼 핑크 제대로 입는 법\n",
      "✅ 크롤링 성공: 저 모델 누구야? 파리에서 발굴한 엘르 보석함\n",
      "✅ 크롤링 성공: 이 조합 진짜예요? 레전드 갱신한 오트 쿠튀르 패션위크 프런트 로\n",
      "✅ 크롤링 성공: 우리 결혼했어요, 런웨이에서!\n",
      "✅ 크롤링 성공: 샤넬이 찾은 추구미의 결정판, 그레이시 에이브럼스\n",
      "✅ 크롤링 성공: 지금 피렌체에 전 세계의 옷 잘 입는 남자들이 모인 이유\n",
      "✅ 크롤링 성공: 패션의 다음 목적지는 이곳입니다\n",
      "✅ 크롤링 성공: 글로벌 패션 컬렉티브(GFC) 2025 S/S 런웨이 하이라이트\n",
      "✅ 크롤링 성공: 천재들의 아파트, 아파트!\n",
      "✅ 크롤링 성공: 2025 S/S 패션위크 프런트로를 빛낸 셀러브리티\n",
      "✅ 크롤링 성공: 니트를 입고 거리로 나선 파리지엔\n",
      "✅ 크롤링 성공: 2025 S/S 패션위크에서 제일 자주 보이는 얼굴\n",
      "✅ 크롤링 성공: 디올 레디 투 웨어 2025 S/S 컬렉션 라이브 스트리밍\n",
      "✅ 크롤링 성공: 석양빛에 물든 구찌라는 낭만\n",
      "✅ 크롤링 성공: 벨라 하디드의 비키니엔 뭔가 특별한 것이 있다\n",
      "✅ 크롤링 성공: 지드래곤과 퍼렐 윌리엄스가 한국에 초특급 행사를 연다\n",
      "✅ 크롤링 성공: 접속, 패션 무비 월드\n",
      "✅ 크롤링 성공: 주름과 볼륨, 올여름 실패 없는 스커트 공식\n",
      "✅ 크롤링 성공: 요즘 아이돌의 스타킹은 다르다\n",
      "✅ 크롤링 성공: 풀오버부터 카디건까지, 모델 곽지영의 '내돈내산' 니트 아이템\n",
      "✅ 크롤링 성공: 가을 스커트에 안 신어주면 서운한 이 슈즈\n",
      "✅ 크롤링 성공: 화이트 셔츠, 이렇게 입어요!\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025-03-07_elle_fashion_trends2.csv\n"
     ]
    }
   ],
   "source": [
    "## 엘르 패션 위크 페이지 크롤링 - 근데 일단 보류 ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL (패션 트렌드 페이지)\n",
    "main_url = \"https://www.elle.co.kr/fashion/fashionweek\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    \n",
    "# 1️⃣ 메인 페이지에서 기사 링크 및 제목 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트\n",
    "article_data = []\n",
    "\n",
    "# 기사 링크와 제목 크롤링\n",
    "for article in soup.select(\"div.atcbox\"):\n",
    "    # 링크 찾기\n",
    "    a_tag = article.select_one(\"a[href^='/article/']\")\n",
    "    if not a_tag:\n",
    "        continue\n",
    "    \n",
    "    article_url = \"https://www.elle.co.kr\" + a_tag[\"href\"]\n",
    "    \n",
    "    # 제목 찾기\n",
    "    title_tag = article.select_one(\"p.tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    article_data.append({\"제목\": title, \"기사 URL\": article_url})  # URL 포함\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견!\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문 크롤링\n",
    "for article in article_data:\n",
    "    title, url = article[\"제목\"], article[\"기사 URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3️⃣ \"에디터\"가 포함된 부분까지만 본문을 저장\n",
    "            editor_tag = soup.find(\"span\", string=re.compile(\"에디터\"))\n",
    "            if editor_tag:\n",
    "                editor_text = editor_tag.get_text(strip=True)  # \"에디터 김영민\" 같은 전체 텍스트\n",
    "                content = content.split(editor_text)[0].strip()  # 해당 부분 이후 본문 삭제\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\"스크래핑 날짜\": scrape_date, \"제목\": title, \"본문\": content, \"기사 URL\": url})\n",
    "            print(f\"✅ 크롤링 성공: {title}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "csv_filename = f\"{scrape_date}_elle_fashion_trends2.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 30개의 기사 링크 발견!\n",
      "✅ 크롤링 성공: Flower Power: 런웨이 꽃밭에 놀러 와 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Sweats: 에디터 스웻 위시리스트 6종 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Fashion in Copenhagen: 우리가 코펜하겐 패션에 끌리는 이유 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Hitting the Slopes: PRADA부터 Chrome Hearts까지.브랜드가 스키 아이템을 만든다면? | 에디터: 김나영\n",
      "✅ 크롤링 성공: Beyond the Logo: 그 많던 로고는 다 어디로 갔을까? | 에디터: 김나영\n",
      "✅ 크롤링 성공: Winter Fleece: 이건 사두면 오래 입겠다 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Furry Club: 털로 무장한 겨울 일기 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Sabotage Street: 자유를 향한 저항정신 | 에디터: Jente Store\n",
      "✅ 크롤링 성공: RETRO ELEGANCE: 우아함을 훔치는 세 가지 방법 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Work Jacket: 바람과 함께 불어온 워크 재킷 열풍 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Women's Boots: 에디터 부츠 위시리스트 6종 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Check Pattern: 세상에 나쁜 체크는 없다 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Camouflage: 꼭꼭 숨어라 카무플라주 보일라 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Running Clothes: 더 달리고 싶게 만들어 줄 러닝 아이템 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Boho Fashion: 자유를 꿈꾸는 보헤미안 스타일 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Grandpacore: 멋쟁이 할아버지 따라잡기 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Socks: 패션의 완성은 양말이다 | 에디터: 김나영\n",
      "✅ 크롤링 성공: 2025 SS Menswear: 응답하라 2025 SS 맨즈웨어 트렌드 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Wear Black In Summer: 애증의 관계, 여름과 블랙 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Fairycore: 요정 뉴진스, 쇠맛 에스파 어디서 온 걸까? | 에디터: 주단단\n",
      "✅ 크롤링 성공: Resort Wear: 우리 여름엔 이렇게 입고 같이 바다 보러 가자 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Bella Hadid's Bag Collection: 벨라 하디드의 뺏어 들고 싶은 백 모음집 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Girl Core: 그녀들의 탐나는 아이템 추적기 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Geek Chic: 당신이 긱 시크에 끌리는 이유 | 에디터: 김나영\n",
      "✅ 크롤링 성공: WHITE ON DENIM: 극도로 발달한 꾸안꾸, 화이트와 데님 | 에디터: 주단단\n",
      "✅ 크롤링 성공: Cowboy Core: 빌딩 숲에서 카우보이 스타일로 살아남기 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Rebels in Fashion: 하지 말라면 더 하고 싶어! | 에디터: 주단단\n",
      "✅ 크롤링 성공: Peach Fuzz: 2024 올해의 컬러 ‘피치 퍼즈’ 이렇게 입으시면 됩니다 | 에디터: 김나영\n",
      "✅ 크롤링 성공: Real Money: 돈 잘 버는 사람들의 옷 입기 공식 해체하기 | 에디터: 주단단\n",
      "✅ 크롤링 성공: 2024 SS Womenswear Collection: 젠테 2024 트렌드 리포트 | 에디터: 주단단\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_jentestore_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Jente Store 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://jentestore.com/promotion?category=trend\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 지정된 형식의 링크 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 링크를 저장할 리스트\n",
    "article_links = []\n",
    "\n",
    "# 지정된 형식의 <a> 태그 찾기 (index=x 형식 링크)\n",
    "for a_tag in soup.select(\"a[href^='/promotion/event_view?no=']\"):\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # 상대 경로를 절대 경로로 변환\n",
    "    full_url = \"https://jentestore.com\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # 중복 제거\n",
    "        article_links.append(full_url)\n",
    "\n",
    "    # 30개까지만 크롤링\n",
    "    if len(article_links) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"🔗 총 {len(article_links)}개의 기사 링크 발견!\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 제목과 본문 크롤링\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 본문을 줄 단위로 분리\n",
    "            lines = content.split(\"\\n\")\n",
    "\n",
    "            # 3️⃣ 제목 설정 (첫 번째 줄:두 번째 줄)\n",
    "            if len(lines) > 1:\n",
    "                title = f\"{lines[0]}: {lines[1]}\"\n",
    "            else:\n",
    "                title = \"제목 없음\"\n",
    "\n",
    "            # 4️⃣ 본문에서 세 번째 줄 까지 제거\n",
    "            content = \"\\n\".join(lines[3:]).strip() if len(lines) > 3 else \"\"\n",
    "\n",
    "             # 5️⃣ \"에디터: 홍길동\" 패턴 추출 & 본문에서 삭제\n",
    "            editor_match = re.search(r\"에디터:\\s*([가-힣]+)\", content)\n",
    "            editor = editor_match.group(1) if editor_match else \"Jente Store\"\n",
    "\n",
    "            # 6️⃣ \"에디터: 홍길동\" 이후 본문 삭제\n",
    "            content = re.split(r\"에디터:\\s*[가-힣]+\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"제목\": title,\n",
    "                \"에디터\": editor,  # 에디터 컬럼 추가\n",
    "                \"본문\": content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 에디터: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"제목\", \"에디터\", \"본문\", \"기사 URL\"])\n",
    "csv_filename = f\"{scrape_date}_jentestore_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 20개의 기사 링크 발견! (최대 30개)\n",
      "✅ 크롤링 성공: 앤아더스토리즈, 세계 여성의 날 기념 실크 스카프 출시! | 업로드 날짜: 2025.03.06 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 2025 F/W 파리 패션위크를 빛낸 스트리트 스타일 | 업로드 날짜: 2025.03.06 | 에디터: 박다연\n",
      "✅ 크롤링 성공: 디올, 세계 여성의 날 기념 다큐멘터리 공개 | 업로드 날짜: 2025.03.06 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 알라이아와 뮈글러, 80년대 패션을 정의한 두 거장의 우정과 유산 | 업로드 날짜: 2025.03.06 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 무라카미 다카시, MLB 도쿄 시리즈 위한 한정판 컬렉션 공개! | 업로드 날짜: 2025.03.06 | 에디터: 박다연\n",
      "✅ 크롤링 성공: 라부부, 글로벌 셀럽들이 주목하는 새로운 패션 아이템? | 업로드 날짜: 2025.03.06 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 밀란패션위크에서 포착한 베스트 스트리트 패션 | 업로드 날짜: 2025.03.05 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 랄프 로렌, MLB 도쿄 시리즈 기념 캡슐 컬렉션 출시! | 업로드 날짜: 2025.03.05 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 리테일 전문가들이 전하는 밀란 패션위크 트렌드 | 업로드 날짜: 2025.03.05 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 2025 F/W 밀란 패션위크, 주목할 만한 데뷔 브랜드 3 | 업로드 날짜: 2025.03.05 | 에디터: 박다연\n",
      "✅ 크롤링 성공: 펜디, 100년의 헤리티지를 담은 런웨이 쇼 | 업로드 날짜: 2025.03.04 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 비욘세와 리바이스의 두 번째 만남 | 업로드 날짜: 2025.03.04 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 제97회 오스카 시상식을 빛낸 베스트 드레서들 | 업로드 날짜: 2025.03.04 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 럭셔리 브랜드들이 선보인 2025 S/S 캠페인 | 업로드 날짜: 2025.02.28 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 패션 일러스트레이터에서 추상 화가로, 타냐 링의 변신 | 업로드 날짜: 2025.02.28 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 글로벌 톱 패션 디자이너가 선정한 2025 맨즈 패션 베스트 아이템 | 업로드 날짜: 2025.02.27 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 헬레나 크리스텐슨, 보컨셉 글로벌 아트 디렉터로 합류 | 업로드 날짜: 2025.02.27 | 에디터: 최주연\n",
      "✅ 크롤링 성공: 전통과 모던함 사이, '위크엔드댄 레더 페니 로퍼' 공개 | 업로드 날짜: 2025.02.26 | 에디터: 김민정\n",
      "✅ 크롤링 성공: 셀럽들의 뿔테 안경 활용법 | 업로드 날짜: 2025.02.26 | 에디터: 이슬\n",
      "✅ 크롤링 성공: 런던패션위크에서 포착한 베스트 스트리트 스타일 | 업로드 날짜: 2025.02.26 | 에디터: 김민정\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_wwdkorea_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## WWD Korea 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://www.wwdkorea.com/news/articleList.html?sc_section_code=S1N3&view_type=sm\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 기사 링크, 제목, 업로드 날짜, 에디터 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트 (최대 30개 크롤링)\n",
    "article_data = []\n",
    "\n",
    "# 지정된 형식의 <a> 태그 찾기 (뉴스 기사 링크)\n",
    "for article in soup.select(\"li div.views\"):\n",
    "    # 기사 링크 찾기\n",
    "    a_tag = article.select_one(\"h4.titles a[href^='/news/articleView.html?idxno=']\")\n",
    "    if not a_tag:\n",
    "        continue  # 링크 없으면 무시\n",
    "\n",
    "    article_url = \"https://www.wwdkorea.com\" + a_tag[\"href\"]\n",
    "\n",
    "    # 기사 제목 찾기 (h4.titles 내부의 a 태그)\n",
    "    title = a_tag.get_text(strip=True) if a_tag else \"제목 없음\"\n",
    "\n",
    "    # 업로드 날짜 찾기\n",
    "    date_tag = article.select_one(\"em.info.dated\")\n",
    "    upload_date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "\n",
    "    # 에디터 찾기\n",
    "    editor_tag = article.select_one(\"em.info.name\")\n",
    "    editor = editor_tag.get_text(strip=True).replace(\" 에디터\", \"\") if editor_tag else \"미상\"\n",
    "\n",
    "    # 기사 정보를 리스트에 추가\n",
    "    article_data.append({\n",
    "        \"제목\": title,\n",
    "        \"기사 URL\": article_url,\n",
    "        \"업로드 날짜\": upload_date,\n",
    "        \"에디터\": editor\n",
    "    })\n",
    "\n",
    "    # 30개까지만 크롤링\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견! (최대 30개)\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문 크롤링\n",
    "for article in article_data:\n",
    "    title, url, upload_date, editor = article[\"제목\"], article[\"기사 URL\"], article[\"업로드 날짜\"], article[\"에디터\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3️⃣ \"<더블유더블유디코리아>가\" 또는 \"<더블유더블유디코리아>의\" 이후 내용 삭제\n",
    "            content = re.split(r\"<더블유더블유디코리아>(가|의)\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"업로드 날짜\": upload_date,\n",
    "                \"에디터\": editor,\n",
    "                \"제목\": title,\n",
    "                \"본문\": content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 업로드 날짜: {upload_date} | 에디터: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일명에 스크래핑 날짜 포함 (예: wwdkorea_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_wwdkorea_trends.csv\"\n",
    "\n",
    "# 4️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"업로드 날짜\", \"에디터\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 10개의 기사 링크 발견! (최대 30개)\n",
      "✅ 크롤링 성공: 2025 S/S 패션 트렌드 #5 | Neo-Bourgeois | 업로드 날짜: 2025.02.25 | 에디터: 이다은\n",
      "✅ 크롤링 성공: 2025 S/S 패션 트렌드 #2 | Belly, On The Sly | 업로드 날짜: 2025.02.25 | 에디터: 김지수\n",
      "✅ 크롤링 성공: 2025 S/S 패션 트렌드 #1 | Layered Elegance | 업로드 날짜: 2025.02.25 | 에디터: 정평화\n",
      "✅ 크롤링 성공: 켄달 제너도 꽂힌 헌팅캡 스타일링 | 업로드 날짜: 2025.02.21 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 납작할수록 스타일리시한 스니커즈 | 업로드 날짜: 2025.02.19 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 제니도 반한 포켓 백 | 업로드 날짜: 2025.02.12 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 목이 허전할 때, 스키니 스카프 | 업로드 날짜: 2025.02.09 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 올봄, 슈트의 시대가 도래할 예정 | 업로드 날짜: 2025.02.06 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 2025 F/W 파리 맨즈 패션위크에 참석한 셀럽은? Part 2 | 업로드 날짜: 2025.01.31 | 에디터: 김 원(프리랜서)\n",
      "✅ 크롤링 성공: 해치지 않아요 | 업로드 날짜: 2025.01.31 | 에디터: 김지수\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_marieclaire_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Marie Claire 패션 트렌드 페이지 크롤링  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://www.marieclairekorea.com/category/fashion/fashion_trend/\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 기사 링크, 제목 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트 (최대 30개 크롤링)\n",
    "article_data = []\n",
    "\n",
    "# 지정된 형식의 <a> 태그 찾기 (기사 링크 & 제목)\n",
    "for article in soup.select(\"h2.entry-title a\"):\n",
    "    article_url = article[\"href\"]\n",
    "    title = article.get_text(strip=True)\n",
    "\n",
    "    # 기사 정보를 리스트에 추가\n",
    "    article_data.append({\n",
    "        \"제목\": title,\n",
    "        \"기사 URL\": article_url\n",
    "    })\n",
    "\n",
    "    # 30개까지만 크롤링\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견! (최대 30개)\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문, 업로드 날짜, 에디터 크롤링\n",
    "for article in article_data:\n",
    "    title, url = article[\"제목\"], article[\"기사 URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 📌 업로드 날짜 추출 (`span.updated.rich-snippet-hidden`)\n",
    "            date_tag = soup.select_one(\"span.updated.rich-snippet-hidden\")\n",
    "            upload_date = date_tag.get_text(strip=True)[:10] if date_tag else \"날짜 없음\" \n",
    "\n",
    "            # 날짜 변환 (YYYY-MM-DD → YYYY.MM.DD)\n",
    "            upload_date = upload_date.replace(\"-\", \".\")\n",
    "\n",
    "            # 📌 에디터 추출 (`span.fn a`)\n",
    "            editor_tag = soup.select_one(\"span.fn a\")\n",
    "            editor = editor_tag.get_text(strip=True) if editor_tag else \"미상\"\n",
    "\n",
    "            # 📌 요약 내용 추출 (`div.mck_post_excerpt`)\n",
    "            excerpt_tag = soup.select_one(\"div.mck_post_excerpt\")\n",
    "            excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else \"\"\n",
    "\n",
    "            # 📌 본문 내용 추출 (`div.post-content p`)\n",
    "            paragraphs = soup.select(\"div.post-content p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 📌 최종 본문 = 요약 + 본문\n",
    "            full_content = f\"{excerpt}\\n{content}\".strip()\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"업로드 날짜\": upload_date,\n",
    "                \"에디터\": editor,\n",
    "                \"제목\": title,\n",
    "                \"본문\": full_content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 업로드 날짜: {upload_date} | 에디터: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일명에 스크래핑 날짜 포함 (예: marieclaire_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_marieclaire_trends.csv\"\n",
    "\n",
    "# 4️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"업로드 날짜\", \"에디터\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔗 총 24개의 기사 링크 발견! (최대 24개)\n",
      "✅ 크롤링 성공: 구찌, 보테가, 샤넬이 아르켜주는 패션 트렌드 | 업로드 날짜: 2025.03.06 | 에디터: 서지현\n",
      "✅ 크롤링 성공: 스웨트 셔츠를 우아하게 즐기는 5가지 방법 | 업로드 날짜: 2025.03.02 | 에디터: 김소연\n",
      "✅ 크롤링 성공: 윈터, 장원영, 조이는 봄마다 ‘이걸’ 입는다? 올봄에는 이 아우터! | 업로드 날짜: 2025.02.25 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: 루이 비통 무라카미 다카시 컬렉션 아이템 화보_최최종 | 업로드 날짜: 2025.02.24 | 에디터: 서지현\n",
      "✅ 크롤링 성공: 프라다, 보테가 베네타, 르메르 등으로 완성한 스포티 룩 5 | 업로드 날짜: 2025.02.22 | 에디터: 서지현\n",
      "✅ 크롤링 성공: 빅백 사고 싶다면? 가성비와 실용성 모두를 갖춘 디자이너 빅백 리스트 5 | 업로드 날짜: 2025.02.19 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: 다니엘, 로제, 민니도 시도한 2025 누드 네일 트렌드 | 업로드 날짜: 2025.02.19 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: 겨울 가고 봄 온다! 봄 아우터 고민은 카리나, 레이 손민수로 해결~ | 업로드 날짜: 2025.02.17 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: MZ세대가 코치를 입는 이유? 뉴욕패션위크에서 포착한 코치 셀링 포인트 4 | 업로드 날짜: 2025.02.17 | 에디터: 최아름\n",
      "✅ 크롤링 성공: Y2K 감성 복링 백, 복서 슈즈 추천 리스트 | 업로드 날짜: 2025.02.17 | 에디터: 전소희\n",
      "✅ 크롤링 성공: 크링클 룩 패션 트렌드는 무엇? 구겨진 옷도 괜찮은 이유 | 업로드 날짜: 2025.02.15 | 에디터: 전소희\n",
      "✅ 크롤링 성공: 밸런타인데이에 받고 싶은 선물 BEST 6 | 업로드 날짜: 2025.02.14 | 에디터: 김소연\n",
      "✅ 크롤링 성공: 요즘 핫하다는 크록스 ‘베이 클로그’ 직접 신어봄! | 업로드 날짜: 2025.02.12 | 에디터: 최아름\n",
      "✅ 크롤링 성공: 크록스와 혜리의 느좋 만남! ‘베이 클로그’ 화보 대 공개 | 업로드 날짜: 2025.02.10 | 에디터: 최아름\n",
      "✅ 크롤링 성공: 친구, 업계 동료, 남매, 부부 동업자의 성공 스토리 4 | 업로드 날짜: 2025.02.10 | 에디터: 서지현\n",
      "✅ 크롤링 성공: 2025 S/S 트렌드 파스텔룩 잘 소화하는 법 | 업로드 날짜: 2025.02.08 | 에디터: 전소희\n",
      "✅ 크롤링 성공: 드뮤어, 걸코어도 아님! 2025 대세 코어는 '캐슬코어' | 업로드 날짜: 2025.02.07 | 에디터: 김소연\n",
      "✅ 크롤링 성공: 2월 생일 선물 추천 3 | 업로드 날짜: 2025.02.03 | 에디터: 조해리\n",
      "✅ 크롤링 성공: 지금이라도 스웨이드 아이템을 사야 하는 이유 | 업로드 날짜: 2025.01.24 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: 살로몬, 레페토 등 신상 슈즈 이렇게 예쁘다고? 그냥 지네가 될게 | 업로드 날짜: 2025.01.20 | 에디터: 송운하\n",
      "✅ 크롤링 성공: 헤일리 비버, 제니는 요즘 레더 재킷을 '이런'핏으로 입는다? 레더 이렇게 입으면 100점! | 업로드 날짜: 2025.01.20 | 에디터: COSMOPOLITAN\n",
      "✅ 크롤링 성공: 시크한 스키룩을 원해? | 업로드 날짜: 2025.01.19 | 에디터: 김미강\n",
      "✅ 크롤링 성공: 부드러움 속에 숨겨진 퍼(Fur)의 강렬함 | 업로드 날짜: 2025.01.19 | 에디터: 김소연\n",
      "✅ 크롤링 성공: 푸른 뱀의 해를 여는 1월의 패션 뉴스 | 업로드 날짜: 2025.01.18 | 에디터: 강아형\n",
      "📂 모든 크롤링 완료! CSV 파일 저장됨: 2025.03.07_cosmopolitan_trends2.csv\n"
     ]
    }
   ],
   "source": [
    "## Cosmopolitan 패션 트렌드 페이지 크롤링  ## --> 아직 보류\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 크롤링할 메인 페이지 URL\n",
    "main_url = \"https://www.cosmopolitan.co.kr/fashion/trends/\"\n",
    "\n",
    "# HTTP 요청 헤더 설정 (봇 차단 방지)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1️⃣ 메인 페이지에서 기사 링크, 제목 가져오기\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# 기사 정보 저장 리스트 (최대 30개 크롤링)\n",
    "article_data = []\n",
    "\n",
    "# 지정된 형식의 <a> 태그 찾기 (기사 링크 & 제목)\n",
    "for article in soup.select(\"div.txtbox\"):\n",
    "    # 기사 제목 찾기\n",
    "    title_tag = article.select_one(\"p.tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"제목 없음\"\n",
    "\n",
    "    # 기사 링크 찾기\n",
    "    a_tag = article.find_parent(\"a\", href=True)\n",
    "    article_url = \"https://www.cosmopolitan.co.kr\" + a_tag[\"href\"] if a_tag else \"링크 없음\"\n",
    "\n",
    "    # 기사 정보를 리스트에 추가\n",
    "    article_data.append({\n",
    "        \"제목\": title,\n",
    "        \"기사 URL\": article_url\n",
    "    })\n",
    "\n",
    "    # 30개까지만 크롤링\n",
    "    if len(article_data) >= 24:\n",
    "        break\n",
    "\n",
    "print(f\"🔗 총 {len(article_data)}개의 기사 링크 발견! (최대 24개)\")\n",
    "\n",
    "# 크롤링 결과를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "# 현재 날짜 가져오기 (스크래핑 날짜)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")  # YYYY.MM.DD 형식\n",
    "\n",
    "# 2️⃣ 각 기사 페이지에서 본문, 업로드 날짜, 에디터 크롤링\n",
    "for article in article_data:\n",
    "    title, url = article[\"제목\"], article[\"기사 URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # 📌 업로드 날짜 추출 (`span.time`)\n",
    "            date_tag = soup.select_one(\"span.time\")\n",
    "            upload_date = date_tag.get_text(strip=True) if date_tag else \"날짜 없음\"\n",
    "\n",
    "            # 📌 에디터 추출 (`a[href^='/editorlist/detail/']`)\n",
    "            editor_tag = soup.select_one(\"a[href^='/editorlist/detail/']\")\n",
    "            editor = editor_tag.get_text(strip=True) if editor_tag else \"미상\"\n",
    "\n",
    "            # 📌 본문 내용 추출 (`p`, `div.ab_photo`) - 특정 형식의 텍스트만 추출 & 특정 태그 이후 크롤링 중지\n",
    "            content_list = []\n",
    "            tag_atc_wrap = soup.select_one(\"div.tag_atc_wrap\")\n",
    "            paragraphs = soup.select(\"p.ab_emphasis_content, p:not([class]), div.ab_photo\")\n",
    "            for p in paragraphs:\n",
    "                if tag_atc_wrap and p in tag_atc_wrap.find_all(\"p\"):\n",
    "                    break\n",
    "                text = p.get_text(strip=True)\n",
    "                content_list.append(text)\n",
    "            content = \"\\n\".join(content_list)\n",
    "\n",
    "\n",
    "            # 리스트에 저장 (URL을 마지막 컬럼으로 배치)\n",
    "            data.append({\n",
    "                \"스크래핑 날짜\": scrape_date,\n",
    "                \"업로드 날짜\": upload_date,\n",
    "                \"에디터\": editor,\n",
    "                \"제목\": title,\n",
    "                \"본문\": content,\n",
    "                \"기사 URL\": url\n",
    "            })\n",
    "            print(f\"✅ 크롤링 성공: {title} | 업로드 날짜: {upload_date} | 에디터: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"⛔ 요청 실패 ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ 오류 발생: {url} - {str(e)}\")\n",
    "\n",
    "# 3️⃣ CSV 파일명에 스크래핑 날짜 포함 (예: cosmopolitan_trends_2025.03.06.csv)\n",
    "csv_filename = f\"{scrape_date}_cosmopolitan_trends2.csv\"\n",
    "\n",
    "# 4️⃣ CSV 파일로 저장 (URL을 마지막 컬럼으로 정렬)\n",
    "df = pd.DataFrame(data, columns=[\"스크래핑 날짜\", \"업로드 날짜\", \"에디터\", \"제목\", \"본문\", \"기사 URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"📂 모든 크롤링 완료! CSV 파일 저장됨: {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
