{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 12ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í‹°ì…”ì¸ ì— ì²­ë°”ì§€, 2025ë…„ì—” â€˜ì´ë ‡ê²Œâ€™ ì…ìœ¼ì„¸ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬í•´ ìš°ë¦¬ì˜ ì˜· ì…ëŠ” ë°©ì‹ì„ ê²°ì •í•  ì‚¬ì§„ 3ì¥ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì¹˜ë§ˆ ì˜ ì…ê³  ì‹¶ë‹¤ë©´ ìƒˆê²¨ë‘ì–´ì•¼ í•  ìš”ì¦˜ ì¡°í•© 6 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 1990ë…„ëŒ€ ê°ì„±ì˜ ì´ íŒ¨í„´ì´ 2025ë…„ ë´„ì— ë§ì¶° ëŒì•„ì™”ìŠµë‹ˆë‹¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ ëª» ì´ê¸°ëŠ” ì²™ ì…ì–´ë³´ê³ í”ˆ, ìŠ¤í‚¤ë‹ˆ ì§„ ì¡°í•© 4 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025ë…„ ë´„, ì„±ê³µì ì¸ ìŠ¤íƒ€ì¼ë§ ì™„ì„±í•  ë¸”ë ˆì´ì € 3 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜ˆìœ ìƒì˜ì˜ ê¸°ì¤€ì´ ë , 2025 ë¯¼ì†Œë§¤ í†± íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°˜ë°”ì§€ì— í° ìš´ë™í™”, ì˜¬ë´„ì— ì…ê³  ì‹¶ì€ ë‚˜ê¸‹í•œ ë£©! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‚®ì„ìˆ˜ë¡ ì¹˜ëª…ì ì¸, ì˜¬ë´„ ìŠ¤ì»¤íŠ¸ ìŠ¤íƒ€ì¼ë§ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°œë ˆì½”ì–´ ë‹¤ìŒ! 2025ë…„ì„ ì •ì˜í•  â€˜ì´ ìŠ¤íƒ€ì¼â€™ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.24\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬í•´ í—ë í•œ ì²­ë°”ì§€ì— ê¼­ ì‹ ì–´ë´ì•¼ í•  ì‹ ë°œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.21\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ ë´„ê³¼ ì—¬ë¦„ ë‚´ë‚´ ëŒë ¤ ì…ê²Œ ë  ì†ì‰¬ìš´ ì›í”¼ìŠ¤ 7 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.21\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_vogue_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## ë³´ê·¸ íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.vogue.co.kr/fashion/fashion-trend/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "article_links = []\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ê°€ í¬í•¨ëœ <a> íƒœê·¸ ì°¾ê¸°\n",
    "for a_tag in soup.select(\"a[href^='/2025']\"):  # 2025ë…„ ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    full_url = \"https://www.vogue.co.kr\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # ì¤‘ë³µ ì œê±°\n",
    "        article_links.append(full_url)\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ì œëª© ì¶”ì¶œ\n",
    "            title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì²« ë²ˆì§¸ ì¤„ ì‚­ì œ, ë‘ ë²ˆì§¸ ì¤„ì„ ì—…ë¡œë“œ ë‚ ì§œë¡œ ì €ì¥\n",
    "            lines = content.split(\"\\n\")  # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            if len(lines) > 1:\n",
    "                upload_date = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„ì´ ì—…ë¡œë“œ ë‚ ì§œ\n",
    "                content = \"\\n\".join(lines[2:]).strip()  # ì²«ì§¸ ì¤„ & ë‘˜ì§¸ ì¤„ ì œê±° í›„ ë³¸ë¬¸ ì¬êµ¬ì„±\n",
    "            else:\n",
    "                upload_date = \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                content = \"\"  # ë‚´ìš©ì´ ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "\n",
    "            # 4ï¸âƒ£ íŠ¹ì • íŒ¨í„´ ì´í›„ ëª¨ë“  ë‚´ìš© ì‚­ì œ\n",
    "            pattern = r\"(íŒ¨ì…˜ ë‰´ìŠ¤|íŒ¨ì…˜ íŠ¸ë Œë“œ|íŒ¨ì…˜ ì•„ì´í…œ|ì—¬í–‰|ì…€ëŸ¬ë¸Œë¦¬í‹° ìŠ¤íƒ€ì¼|ì›°ë‹ˆìŠ¤|ë·° í¬ì¸íŠ¸)\\n\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[ê°€-í£]+.*\"\n",
    "            content = re.sub(pattern, \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_vogue_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 15ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ íŠ¸ë Œì¹˜ì½”íŠ¸ëŠ” ì§§ì€ ê¸¸ì´ê°€ ì •ë‹µì…ë‹ˆë‹¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í‹°ëª¨ì‹œë¥¼ ìœ„í•œ ì¹´ì¼ë¦¬ ì œë„ˆì˜ ê´€ëŠ¥ì ì¸ ë“œë ˆìŠ¤ 3ì¢… ì„¸íŠ¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¶€ë‹´ ì—†ì´ ë„ì „í•˜ëŠ” ì˜¬ ë ˆë“œ ë£© ì—°ì¶œë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬í•´ ì›¨ì§€ ìŠˆì¦ˆê°€ ìœ í–‰ì´ë¼ëŠ” ì†Œë¬¸ì€ ì§„ì§œë‹¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 'ê¾¸ì•ˆê¾¸ ë§ˆìŠ¤í„°' ì¹´ì´ì•„ ê±°ë²„ì˜ ë³¼ìº¡ í™œìš©ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê°€ì¥ ì‰½ê²Œ ì„¸ë ¨ëœ ë°©ì‹ìœ¼ë¡œ ì¬í‚·ì„ ì…ëŠ” ë²• (ì œë‹ˆ VER.) | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ ê°€ì¥ ì‹¤ì† ìˆëŠ” ì¡°í•©, ì…”ì¸ ì— ë² ìŠ¤íŠ¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‹¤ë¦¬ê°€ ê¸¸ì–´ë³´ì´ëŠ” ì œë‹ˆì˜ í•˜ì˜ ì‹¤ì¢… ë£© ê³µì‹ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¥´ì„¸ë¼í•Œ ê¹€ì±„ì›, ë”ë¸”ìœ  ë””ì§€í„¸ ì»¤ë²„ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ìš°ì•„í•˜ê³ ë„ ë¶€ë“œëŸ¬ìš´ ì˜¬ë´„ ì¬í‚· ì»¬ëŸ¬ëŠ”? | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê·¸ëŒ€ë¡œ ë”°ë¼ ì…ê³  ì‹¶ì€ ëª¨ë¸ 3ì¸ì˜ ë´„ ì•„ì›ƒí• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¡œë§¨í‹± vs ìºì£¼ì–¼, ì‚°ëœ»í•œ ë´„ë‚˜ë“¤ì´ ë£©ì„ ìœ„í•œ íŒ¬ì¸  2 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ì—” 'ì´ê²ƒ'ë§Œ ë§¤ë©´ ìŠ¤íƒ€ì¼ë¦¬ì‹œí•´ ì ¸ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê°œì¸ ë””ìì´ë„ˆ ë¸Œëœë“œë¡œ ì™„ì„±í•œ ë¦¬ì‚¬ì˜ ì˜¤ìŠ¤ì¹´ ëª¨ë¨¼íŠ¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì œ 97íšŒ ì˜¤ìŠ¤ì¹´ ì‹œìƒì‹ì„ ë¹›ë‚¸ ìƒ¤ë„¬ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_wkorea_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Wì½”ë¦¬ì•„ íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.wkorea.com/fashion/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ë° ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "article_data = []\n",
    "\n",
    "# ê¸°ì‚¬ ëª©ë¡ í¬ë¡¤ë§ (`li` íƒœê·¸ ê¸°ì¤€ìœ¼ë¡œ íƒìƒ‰)\n",
    "for article in soup.select(\"li\"):\n",
    "    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (`a` íƒœê·¸ ë‚´ë¶€ì˜ href ì†ì„±)\n",
    "    a_tag = article.select_one(\"a[href^='https://www.wkorea.com/2025']\")\n",
    "    if not a_tag:\n",
    "        continue  # ê¸°ì‚¬ ë§í¬ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    article_url = a_tag[\"href\"]\n",
    "\n",
    "    # ê¸°ì‚¬ ì œëª© ì°¾ê¸° (`h3.s-tit` íƒœê·¸ ë‚´ë¶€)\n",
    "    title_tag = article.select_one(\"h3.s_tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ URLê³¼ ì œëª©ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\"ì œëª©\": title, \"ê¸°ì‚¬ URL\": article_url})\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§ & ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ\n",
    "for article in article_data:\n",
    "    title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "             # 3ï¸âƒ£ ë³¸ë¬¸ ì²« ì¤„ & ë‘ ë²ˆì§¸ ì¤„ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ & ì—ë””í„° ì¶”ì¶œ\n",
    "            lines = content.split(\"\\n\")  # ë³¸ë¬¸ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            if len(lines) > 1:\n",
    "                # \"W Fashion YYYY.MM.DD\" í˜•ì‹ì—ì„œ \"W Fashion\" ì œê±° í›„ ë‚ ì§œë§Œ ì €ì¥\n",
    "                upload_date_match = re.match(r\"W Fashion\\s*(\\d{4}\\.\\d{2}\\.\\d{2})\", lines[0])\n",
    "                upload_date = upload_date_match.group(1) if upload_date_match else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                \n",
    "                editor = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„ (ì—ë””í„°)\n",
    "            else:\n",
    "                upload_date = \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                editor = \"ë¯¸ìƒ\"\n",
    "\n",
    "            # 4ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì²« ë‘ ì¤„(ì—…ë¡œë“œ ë‚ ì§œ & ì—ë””í„°) ì œê±°\n",
    "            content = \"\\n\".join(lines[2:]).strip() if len(lines) > 2 else \"\"\n",
    "\n",
    "            # 5ï¸âƒ£ ë³¸ë¬¸ ëë¶€ë¶„ì˜ \"W Fashion YYYY.MM.DD by ì´ë¦„\" íŒ¨í„´ ì œê±°\n",
    "            content = re.sub(r\"W\\s*Fashion\\n*\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[ê°€-í£]+.*\", \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 6ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_wkorea_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 30ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Flower Power: ëŸ°ì›¨ì´ ê½ƒë°­ì— ë†€ëŸ¬ ì™€ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Sweats: ì—ë””í„° ìŠ¤ì›» ìœ„ì‹œë¦¬ìŠ¤íŠ¸ 6ì¢… | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Fashion in Copenhagen: ìš°ë¦¬ê°€ ì½”íœí•˜ê² íŒ¨ì…˜ì— ëŒë¦¬ëŠ” ì´ìœ  | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Hitting the Slopes: PRADAë¶€í„° Chrome Heartsê¹Œì§€.ë¸Œëœë“œê°€ ìŠ¤í‚¤ ì•„ì´í…œì„ ë§Œë“ ë‹¤ë©´? | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Beyond the Logo: ê·¸ ë§ë˜ ë¡œê³ ëŠ” ë‹¤ ì–´ë””ë¡œ ê°”ì„ê¹Œ? | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Winter Fleece: ì´ê±´ ì‚¬ë‘ë©´ ì˜¤ë˜ ì…ê² ë‹¤ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Furry Club: í„¸ë¡œ ë¬´ì¥í•œ ê²¨ìš¸ ì¼ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Sabotage Street: ììœ ë¥¼ í–¥í•œ ì €í•­ì •ì‹  | ì—ë””í„°: Jente Store\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: RETRO ELEGANCE: ìš°ì•„í•¨ì„ í›”ì¹˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²• | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Work Jacket: ë°”ëŒê³¼ í•¨ê»˜ ë¶ˆì–´ì˜¨ ì›Œí¬ ì¬í‚· ì—´í’ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Women's Boots: ì—ë””í„° ë¶€ì¸  ìœ„ì‹œë¦¬ìŠ¤íŠ¸ 6ì¢… | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Check Pattern: ì„¸ìƒì— ë‚˜ìœ ì²´í¬ëŠ” ì—†ë‹¤ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Camouflage: ê¼­ê¼­ ìˆ¨ì–´ë¼ ì¹´ë¬´í”Œë¼ì£¼ ë³´ì¼ë¼ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Running Clothes: ë” ë‹¬ë¦¬ê³  ì‹¶ê²Œ ë§Œë“¤ì–´ ì¤„ ëŸ¬ë‹ ì•„ì´í…œ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Boho Fashion: ììœ ë¥¼ ê¿ˆê¾¸ëŠ” ë³´í—¤ë¯¸ì•ˆ ìŠ¤íƒ€ì¼ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Grandpacore: ë©‹ìŸì´ í• ì•„ë²„ì§€ ë”°ë¼ì¡ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Socks: íŒ¨ì…˜ì˜ ì™„ì„±ì€ ì–‘ë§ì´ë‹¤ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 SS Menswear: ì‘ë‹µí•˜ë¼ 2025 SS ë§¨ì¦ˆì›¨ì–´ íŠ¸ë Œë“œ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Wear Black In Summer: ì• ì¦ì˜ ê´€ê³„, ì—¬ë¦„ê³¼ ë¸”ë™ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Fairycore: ìš”ì • ë‰´ì§„ìŠ¤, ì‡ ë§› ì—ìŠ¤íŒŒ ì–´ë””ì„œ ì˜¨ ê±¸ê¹Œ? | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Resort Wear: ìš°ë¦¬ ì—¬ë¦„ì—” ì´ë ‡ê²Œ ì…ê³  ê°™ì´ ë°”ë‹¤ ë³´ëŸ¬ ê°€ì | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Bella Hadid's Bag Collection: ë²¨ë¼ í•˜ë””ë“œì˜ ëºì–´ ë“¤ê³  ì‹¶ì€ ë°± ëª¨ìŒì§‘ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Girl Core: ê·¸ë…€ë“¤ì˜ íƒë‚˜ëŠ” ì•„ì´í…œ ì¶”ì ê¸° | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Geek Chic: ë‹¹ì‹ ì´ ê¸± ì‹œí¬ì— ëŒë¦¬ëŠ” ì´ìœ  | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: WHITE ON DENIM: ê·¹ë„ë¡œ ë°œë‹¬í•œ ê¾¸ì•ˆê¾¸, í™”ì´íŠ¸ì™€ ë°ë‹˜ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Cowboy Core: ë¹Œë”© ìˆ²ì—ì„œ ì¹´ìš°ë³´ì´ ìŠ¤íƒ€ì¼ë¡œ ì‚´ì•„ë‚¨ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Rebels in Fashion: í•˜ì§€ ë§ë¼ë©´ ë” í•˜ê³  ì‹¶ì–´! | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Peach Fuzz: 2024 ì˜¬í•´ì˜ ì»¬ëŸ¬ â€˜í”¼ì¹˜ í¼ì¦ˆâ€™ ì´ë ‡ê²Œ ì…ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Real Money: ëˆ ì˜ ë²„ëŠ” ì‚¬ëŒë“¤ì˜ ì˜· ì…ê¸° ê³µì‹ í•´ì²´í•˜ê¸° | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2024 SS Womenswear Collection: ì  í…Œ 2024 íŠ¸ë Œë“œ ë¦¬í¬íŠ¸ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_jentestore_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Jente Store íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://jentestore.com/promotion?category=trend\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì§€ì •ëœ í˜•ì‹ì˜ ë§í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "article_links = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (index=x í˜•ì‹ ë§í¬)\n",
    "for a_tag in soup.select(\"a[href^='/promotion/event_view?no=']\"):\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    full_url = \"https://jentestore.com\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # ì¤‘ë³µ ì œê±°\n",
    "        article_links.append(full_url)\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_links) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # ë³¸ë¬¸ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            lines = content.split(\"\\n\")\n",
    "\n",
    "            # 3ï¸âƒ£ ì œëª© ì„¤ì • (ì²« ë²ˆì§¸ ì¤„:ë‘ ë²ˆì§¸ ì¤„)\n",
    "            if len(lines) > 1:\n",
    "                title = f\"{lines[0]}: {lines[1]}\"\n",
    "            else:\n",
    "                title = \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "            # 4ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì„¸ ë²ˆì§¸ ì¤„ ê¹Œì§€ ì œê±°\n",
    "            content = \"\\n\".join(lines[3:]).strip() if len(lines) > 3 else \"\"\n",
    "\n",
    "             # 5ï¸âƒ£ \"ì—ë””í„°: í™ê¸¸ë™\" íŒ¨í„´ ì¶”ì¶œ & ë³¸ë¬¸ì—ì„œ ì‚­ì œ\n",
    "            editor_match = re.search(r\"ì—ë””í„°:\\s*([ê°€-í£]+)\", content)\n",
    "            editor = editor_match.group(1) if editor_match else \"Jente Store\"\n",
    "\n",
    "            # 6ï¸âƒ£ \"ì—ë””í„°: í™ê¸¸ë™\" ì´í›„ ë³¸ë¬¸ ì‚­ì œ\n",
    "            content = re.split(r\"ì—ë””í„°:\\s*[ê°€-í£]+\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ì—ë””í„°\": editor,  # ì—ë””í„° ì»¬ëŸ¼ ì¶”ê°€\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì œëª©\", \"ì—ë””í„°\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_jentestore_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 20ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì•¤ì•„ë”ìŠ¤í† ë¦¬ì¦ˆ, ì„¸ê³„ ì—¬ì„±ì˜ ë‚  ê¸°ë… ì‹¤í¬ ìŠ¤ì¹´í”„ ì¶œì‹œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W íŒŒë¦¬ íŒ¨ì…˜ìœ„í¬ë¥¼ ë¹›ë‚¸ ìŠ¤íŠ¸ë¦¬íŠ¸ ìŠ¤íƒ€ì¼ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë””ì˜¬, ì„¸ê³„ ì—¬ì„±ì˜ ë‚  ê¸°ë… ë‹¤íë©˜í„°ë¦¬ ê³µê°œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì•Œë¼ì´ì•„ì™€ ë®ˆê¸€ëŸ¬, 80ë…„ëŒ€ íŒ¨ì…˜ì„ ì •ì˜í•œ ë‘ ê±°ì¥ì˜ ìš°ì •ê³¼ ìœ ì‚° | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¬´ë¼ì¹´ë¯¸ ë‹¤ì¹´ì‹œ, MLB ë„ì¿„ ì‹œë¦¬ì¦ˆ ìœ„í•œ í•œì •íŒ ì»¬ë ‰ì…˜ ê³µê°œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¼ë¶€ë¶€, ê¸€ë¡œë²Œ ì…€ëŸ½ë“¤ì´ ì£¼ëª©í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ì…˜ ì•„ì´í…œ? | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°€ë€íŒ¨ì…˜ìœ„í¬ì—ì„œ í¬ì°©í•œ ë² ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬íŠ¸ íŒ¨ì…˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë„í”„ ë¡œë Œ, MLB ë„ì¿„ ì‹œë¦¬ì¦ˆ ê¸°ë… ìº¡ìŠ ì»¬ë ‰ì…˜ ì¶œì‹œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¦¬í…Œì¼ ì „ë¬¸ê°€ë“¤ì´ ì „í•˜ëŠ” ë°€ë€ íŒ¨ì…˜ìœ„í¬ íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W ë°€ë€ íŒ¨ì…˜ìœ„í¬, ì£¼ëª©í•  ë§Œí•œ ë°ë·” ë¸Œëœë“œ 3 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: íœë””, 100ë…„ì˜ í—¤ë¦¬í‹°ì§€ë¥¼ ë‹´ì€ ëŸ°ì›¨ì´ ì‡¼ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¹„ìš˜ì„¸ì™€ ë¦¬ë°”ì´ìŠ¤ì˜ ë‘ ë²ˆì§¸ ë§Œë‚¨ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì œ97íšŒ ì˜¤ìŠ¤ì¹´ ì‹œìƒì‹ì„ ë¹›ë‚¸ ë² ìŠ¤íŠ¸ ë“œë ˆì„œë“¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëŸ­ì…”ë¦¬ ë¸Œëœë“œë“¤ì´ ì„ ë³´ì¸ 2025 S/S ìº í˜ì¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: íŒ¨ì…˜ ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ ì¶”ìƒ í™”ê°€ë¡œ, íƒ€ëƒ ë§ì˜ ë³€ì‹  | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê¸€ë¡œë²Œ í†± íŒ¨ì…˜ ë””ìì´ë„ˆê°€ ì„ ì •í•œ 2025 ë§¨ì¦ˆ íŒ¨ì…˜ ë² ìŠ¤íŠ¸ ì•„ì´í…œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í—¬ë ˆë‚˜ í¬ë¦¬ìŠ¤í…ìŠ¨, ë³´ì»¨ì…‰ ê¸€ë¡œë²Œ ì•„íŠ¸ ë””ë ‰í„°ë¡œ í•©ë¥˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì „í†µê³¼ ëª¨ë˜í•¨ ì‚¬ì´, 'ìœ„í¬ì—”ë“œëŒ„ ë ˆë” í˜ë‹ˆ ë¡œí¼' ê³µê°œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì…€ëŸ½ë“¤ì˜ ë¿”í…Œ ì•ˆê²½ í™œìš©ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26 | ì—ë””í„°: ì´ìŠ¬\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëŸ°ë˜íŒ¨ì…˜ìœ„í¬ì—ì„œ í¬ì°©í•œ ë² ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬íŠ¸ ìŠ¤íƒ€ì¼ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_wwdkorea_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## WWD Korea íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.wwdkorea.com/news/articleList.html?sc_section_code=S1N3&view_type=sm\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª©, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "article_data = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬)\n",
    "for article in soup.select(\"li div.views\"):\n",
    "    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°\n",
    "    a_tag = article.select_one(\"h4.titles a[href^='/news/articleView.html?idxno=']\")\n",
    "    if not a_tag:\n",
    "        continue  # ë§í¬ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    article_url = \"https://www.wwdkorea.com\" + a_tag[\"href\"]\n",
    "\n",
    "    # ê¸°ì‚¬ ì œëª© ì°¾ê¸° (h4.titles ë‚´ë¶€ì˜ a íƒœê·¸)\n",
    "    title = a_tag.get_text(strip=True) if a_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ì—…ë¡œë“œ ë‚ ì§œ ì°¾ê¸°\n",
    "    date_tag = article.select_one(\"em.info.dated\")\n",
    "    upload_date = date_tag.get_text(strip=True) if date_tag else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "\n",
    "    # ì—ë””í„° ì°¾ê¸°\n",
    "    editor_tag = article.select_one(\"em.info.name\")\n",
    "    editor = editor_tag.get_text(strip=True).replace(\" ì—ë””í„°\", \"\") if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\n",
    "        \"ì œëª©\": title,\n",
    "        \"ê¸°ì‚¬ URL\": article_url,\n",
    "        \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "        \"ì—ë””í„°\": editor\n",
    "    })\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for article in article_data:\n",
    "    title, url, upload_date, editor = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"], article[\"ì—…ë¡œë“œ ë‚ ì§œ\"], article[\"ì—ë””í„°\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3ï¸âƒ£ \"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>ê°€\" ë˜ëŠ” \"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>ì˜\" ì´í›„ ë‚´ìš© ì‚­ì œ\n",
    "            content = re.split(r\"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>(ê°€|ì˜)\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: wwdkorea_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_wwdkorea_trends.csv\"\n",
    "\n",
    "# 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 10ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #5 | Neo-Bourgeois | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ì´ë‹¤ì€\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #2 | Belly, On The Sly | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ê¹€ì§€ìˆ˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #1 | Layered Elegance | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ì •í‰í™”\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì¼„ë‹¬ ì œë„ˆë„ ê½‚íŒ í—ŒíŒ…ìº¡ ìŠ¤íƒ€ì¼ë§ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.21 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‚©ì‘í• ìˆ˜ë¡ ìŠ¤íƒ€ì¼ë¦¬ì‹œí•œ ìŠ¤ë‹ˆì»¤ì¦ˆ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.19 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì œë‹ˆë„ ë°˜í•œ í¬ì¼“ ë°± | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.12 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëª©ì´ í—ˆì „í•  ë•Œ, ìŠ¤í‚¤ë‹ˆ ìŠ¤ì¹´í”„ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.09 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„, ìŠˆíŠ¸ì˜ ì‹œëŒ€ê°€ ë„ë˜í•  ì˜ˆì • | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.06 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W íŒŒë¦¬ ë§¨ì¦ˆ íŒ¨ì…˜ìœ„í¬ì— ì°¸ì„í•œ ì…€ëŸ½ì€? Part 2 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.31 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í•´ì¹˜ì§€ ì•Šì•„ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.31 | ì—ë””í„°: ê¹€ì§€ìˆ˜\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_marieclaire_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Marie Claire íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.marieclairekorea.com/category/fashion/fashion_trend/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "article_data = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ê¸°ì‚¬ ë§í¬ & ì œëª©)\n",
    "for article in soup.select(\"h2.entry-title a\"):\n",
    "    article_url = article[\"href\"]\n",
    "    title = article.get_text(strip=True)\n",
    "\n",
    "    # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\n",
    "        \"ì œëª©\": title,\n",
    "        \"ê¸°ì‚¬ URL\": article_url\n",
    "    })\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° í¬ë¡¤ë§\n",
    "for article in article_data:\n",
    "    title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ğŸ“Œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (`span.updated.rich-snippet-hidden`)\n",
    "            date_tag = soup.select_one(\"span.updated.rich-snippet-hidden\")\n",
    "            upload_date = date_tag.get_text(strip=True)[:10] if date_tag else \"ë‚ ì§œ ì—†ìŒ\" \n",
    "\n",
    "            # ë‚ ì§œ ë³€í™˜ (YYYY-MM-DD â†’ YYYY.MM.DD)\n",
    "            upload_date = upload_date.replace(\"-\", \".\")\n",
    "\n",
    "            # ğŸ“Œ ì—ë””í„° ì¶”ì¶œ (`span.fn a`)\n",
    "            editor_tag = soup.select_one(\"span.fn a\")\n",
    "            editor = editor_tag.get_text(strip=True) if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "            # ğŸ“Œ ìš”ì•½ ë‚´ìš© ì¶”ì¶œ (`div.mck_post_excerpt`)\n",
    "            excerpt_tag = soup.select_one(\"div.mck_post_excerpt\")\n",
    "            excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else \"\"\n",
    "\n",
    "            # ğŸ“Œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (`div.post-content p`)\n",
    "            paragraphs = soup.select(\"div.post-content p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # ğŸ“Œ ìµœì¢… ë³¸ë¬¸ = ìš”ì•½ + ë³¸ë¬¸\n",
    "            full_content = f\"{excerpt}\\n{content}\".strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": full_content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: marieclaire_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_marieclaire_trends.csv\"\n",
    "\n",
    "# 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 24ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 24ê°œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: êµ¬ì°Œ, ë³´í…Œê°€, ìƒ¤ë„¬ì´ ì•„ë¥´ì¼œì£¼ëŠ” íŒ¨ì…˜ íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ì„œì§€í˜„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ìŠ¤ì›¨íŠ¸ ì…”ì¸ ë¥¼ ìš°ì•„í•˜ê²Œ ì¦ê¸°ëŠ” 5ê°€ì§€ ë°©ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.02 | ì—ë””í„°: ê¹€ì†Œì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ìœˆí„°, ì¥ì›ì˜, ì¡°ì´ëŠ” ë´„ë§ˆë‹¤ â€˜ì´ê±¸â€™ ì…ëŠ”ë‹¤? ì˜¬ë´„ì—ëŠ” ì´ ì•„ìš°í„°! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë£¨ì´ ë¹„í†µ ë¬´ë¼ì¹´ë¯¸ ë‹¤ì¹´ì‹œ ì»¬ë ‰ì…˜ ì•„ì´í…œ í™”ë³´_ìµœìµœì¢… | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.24 | ì—ë””í„°: ì„œì§€í˜„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í”„ë¼ë‹¤, ë³´í…Œê°€ ë² ë„¤íƒ€, ë¥´ë©”ë¥´ ë“±ìœ¼ë¡œ ì™„ì„±í•œ ìŠ¤í¬í‹° ë£© 5 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.22 | ì—ë””í„°: ì„œì§€í˜„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¹…ë°± ì‚¬ê³  ì‹¶ë‹¤ë©´? ê°€ì„±ë¹„ì™€ ì‹¤ìš©ì„± ëª¨ë‘ë¥¼ ê°–ì¶˜ ë””ìì´ë„ˆ ë¹…ë°± ë¦¬ìŠ¤íŠ¸ 5 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.19 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‹¤ë‹ˆì—˜, ë¡œì œ, ë¯¼ë‹ˆë„ ì‹œë„í•œ 2025 ëˆ„ë“œ ë„¤ì¼ íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.19 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê²¨ìš¸ ê°€ê³  ë´„ ì˜¨ë‹¤! ë´„ ì•„ìš°í„° ê³ ë¯¼ì€ ì¹´ë¦¬ë‚˜, ë ˆì´ ì†ë¯¼ìˆ˜ë¡œ í•´ê²°~ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.17 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: MZì„¸ëŒ€ê°€ ì½”ì¹˜ë¥¼ ì…ëŠ” ì´ìœ ? ë‰´ìš•íŒ¨ì…˜ìœ„í¬ì—ì„œ í¬ì°©í•œ ì½”ì¹˜ ì…€ë§ í¬ì¸íŠ¸ 4 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.17 | ì—ë””í„°: ìµœì•„ë¦„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Y2K ê°ì„± ë³µë§ ë°±, ë³µì„œ ìŠˆì¦ˆ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.17 | ì—ë””í„°: ì „ì†Œí¬\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í¬ë§í´ ë£© íŒ¨ì…˜ íŠ¸ë Œë“œëŠ” ë¬´ì—‡? êµ¬ê²¨ì§„ ì˜·ë„ ê´œì°®ì€ ì´ìœ  | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.15 | ì—ë””í„°: ì „ì†Œí¬\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°¸ëŸ°íƒ€ì¸ë°ì´ì— ë°›ê³  ì‹¶ì€ ì„ ë¬¼ BEST 6 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.14 | ì—ë””í„°: ê¹€ì†Œì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ìš”ì¦˜ í•«í•˜ë‹¤ëŠ” í¬ë¡ìŠ¤ â€˜ë² ì´ í´ë¡œê·¸â€™ ì§ì ‘ ì‹ ì–´ë´„! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.12 | ì—ë””í„°: ìµœì•„ë¦„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í¬ë¡ìŠ¤ì™€ í˜œë¦¬ì˜ ëŠì¢‹ ë§Œë‚¨! â€˜ë² ì´ í´ë¡œê·¸â€™ í™”ë³´ ëŒ€ ê³µê°œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.10 | ì—ë””í„°: ìµœì•„ë¦„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì¹œêµ¬, ì—…ê³„ ë™ë£Œ, ë‚¨ë§¤, ë¶€ë¶€ ë™ì—…ìì˜ ì„±ê³µ ìŠ¤í† ë¦¬ 4 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.10 | ì—ë””í„°: ì„œì§€í˜„\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŠ¸ë Œë“œ íŒŒìŠ¤í…”ë£© ì˜ ì†Œí™”í•˜ëŠ” ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.08 | ì—ë””í„°: ì „ì†Œí¬\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë“œë®¤ì–´, ê±¸ì½”ì–´ë„ ì•„ë‹˜! 2025 ëŒ€ì„¸ ì½”ì–´ëŠ” 'ìºìŠ¬ì½”ì–´' | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.07 | ì—ë””í„°: ê¹€ì†Œì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2ì›” ìƒì¼ ì„ ë¬¼ ì¶”ì²œ 3 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.03 | ì—ë””í„°: ì¡°í•´ë¦¬\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì§€ê¸ˆì´ë¼ë„ ìŠ¤ì›¨ì´ë“œ ì•„ì´í…œì„ ì‚¬ì•¼ í•˜ëŠ” ì´ìœ  | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.24 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì‚´ë¡œëª¬, ë ˆí˜í†  ë“± ì‹ ìƒ ìŠˆì¦ˆ ì´ë ‡ê²Œ ì˜ˆì˜ë‹¤ê³ ? ê·¸ëƒ¥ ì§€ë„¤ê°€ ë ê²Œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.20 | ì—ë””í„°: ì†¡ìš´í•˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í—¤ì¼ë¦¬ ë¹„ë²„, ì œë‹ˆëŠ” ìš”ì¦˜ ë ˆë” ì¬í‚·ì„ 'ì´ëŸ°'í•ìœ¼ë¡œ ì…ëŠ”ë‹¤? ë ˆë” ì´ë ‡ê²Œ ì…ìœ¼ë©´ 100ì ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.20 | ì—ë””í„°: COSMOPOLITAN\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì‹œí¬í•œ ìŠ¤í‚¤ë£©ì„ ì›í•´? | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.19 | ì—ë””í„°: ê¹€ë¯¸ê°•\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¶€ë“œëŸ¬ì›€ ì†ì— ìˆ¨ê²¨ì§„ í¼(Fur)ì˜ ê°•ë ¬í•¨ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.19 | ì—ë””í„°: ê¹€ì†Œì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í‘¸ë¥¸ ë±€ì˜ í•´ë¥¼ ì—¬ëŠ” 1ì›”ì˜ íŒ¨ì…˜ ë‰´ìŠ¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.18 | ì—ë””í„°: ê°•ì•„í˜•\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.07_cosmopolitan_trends2.csv\n"
     ]
    }
   ],
   "source": [
    "## Cosmopolitan íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ## --> ì•„ì§ ë³´ë¥˜\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.cosmopolitan.co.kr/fashion/trends/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "article_data = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ê¸°ì‚¬ ë§í¬ & ì œëª©)\n",
    "for article in soup.select(\"div.txtbox\"):\n",
    "    # ê¸°ì‚¬ ì œëª© ì°¾ê¸°\n",
    "    title_tag = article.select_one(\"p.tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°\n",
    "    a_tag = article.find_parent(\"a\", href=True)\n",
    "    article_url = \"https://www.cosmopolitan.co.kr\" + a_tag[\"href\"] if a_tag else \"ë§í¬ ì—†ìŒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\n",
    "        \"ì œëª©\": title,\n",
    "        \"ê¸°ì‚¬ URL\": article_url\n",
    "    })\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_data) >= 24:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 24ê°œ)\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")  # YYYY.MM.DD í˜•ì‹\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° í¬ë¡¤ë§\n",
    "for article in article_data:\n",
    "    title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ğŸ“Œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (`span.time`)\n",
    "            date_tag = soup.select_one(\"span.time\")\n",
    "            upload_date = date_tag.get_text(strip=True) if date_tag else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "\n",
    "            # ğŸ“Œ ì—ë””í„° ì¶”ì¶œ (`a[href^='/editorlist/detail/']`)\n",
    "            editor_tag = soup.select_one(\"a[href^='/editorlist/detail/']\")\n",
    "            editor = editor_tag.get_text(strip=True) if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "            # ğŸ“Œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (`p`, `div.ab_photo`) - íŠ¹ì • í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ & íŠ¹ì • íƒœê·¸ ì´í›„ í¬ë¡¤ë§ ì¤‘ì§€\n",
    "            content_list = []\n",
    "            tag_atc_wrap = soup.select_one(\"div.tag_atc_wrap\")\n",
    "            paragraphs = soup.select(\"p.ab_emphasis_content, p:not([class]), div.ab_photo\")\n",
    "            for p in paragraphs:\n",
    "                if tag_atc_wrap and p in tag_atc_wrap.find_all(\"p\"):\n",
    "                    break\n",
    "                text = p.get_text(strip=True)\n",
    "                content_list.append(text)\n",
    "            content = \"\\n\".join(content_list)\n",
    "\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: cosmopolitan_trends_2025.03.06.csv)\n",
    "csv_filename = f\"{scrape_date}_cosmopolitan_trends2.csv\"\n",
    "\n",
    "# 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
