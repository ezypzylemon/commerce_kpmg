{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 12ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëª¨ë‘ê°€ ë˜‘ê°™ì€ ì˜·ì„ ì…ëŠ” ì‹œëŒ€, ì§„ì§œ ë©‹ì€ ì–´ë””ì— ìˆë‚˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.07\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ìŠ¤í‚¤ë‹ˆ ì§„ì„ ì…ì§€ ì•ŠëŠ” ì‚¬ëŒë“¤ì„ ìœ„í•œ ìŠ¤í‚¤ë‹ˆ ì§„ ì…ëŠ” ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.07\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í‹°ì…”ì¸ ì— ì²­ë°”ì§€, 2025ë…„ì—” â€˜ì´ë ‡ê²Œâ€™ ì…ìœ¼ì„¸ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬í•´ ìš°ë¦¬ì˜ ì˜· ì…ëŠ” ë°©ì‹ì„ ê²°ì •í•  ì‚¬ì§„ 3ì¥ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì¹˜ë§ˆ ì˜ ì…ê³  ì‹¶ë‹¤ë©´ ìƒˆê²¨ë‘ì–´ì•¼ í•  ìš”ì¦˜ ì¡°í•© 6 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 1990ë…„ëŒ€ ê°ì„±ì˜ ì´ íŒ¨í„´ì´ 2025ë…„ ë´„ì— ë§ì¶° ëŒì•„ì™”ìŠµë‹ˆë‹¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ ëª» ì´ê¸°ëŠ” ì²™ ì…ì–´ë³´ê³ í”ˆ, ìŠ¤í‚¤ë‹ˆ ì§„ ì¡°í•© 4 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025ë…„ ë´„, ì„±ê³µì ì¸ ìŠ¤íƒ€ì¼ë§ ì™„ì„±í•  ë¸”ë ˆì´ì € 3 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜ˆìœ ìƒì˜ì˜ ê¸°ì¤€ì´ ë , 2025 ë¯¼ì†Œë§¤ í†± íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.26\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°˜ë°”ì§€ì— í° ìš´ë™í™”, ì˜¬ë´„ì— ì…ê³  ì‹¶ì€ ë‚˜ê¸‹í•œ ë£©! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‚®ì„ìˆ˜ë¡ ì¹˜ëª…ì ì¸, ì˜¬ë´„ ìŠ¤ì»¤íŠ¸ ìŠ¤íƒ€ì¼ë§ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°œë ˆì½”ì–´ ë‹¤ìŒ! 2025ë…„ì„ ì •ì˜í•  â€˜ì´ ìŠ¤íƒ€ì¼â€™ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.24\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.11_vogue_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## ë³´ê·¸ íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.vogue.co.kr/fashion/fashion-trend/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "article_links = []\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ê°€ í¬í•¨ëœ <a> íƒœê·¸ ì°¾ê¸°\n",
    "for a_tag in soup.select(\"a[href^='/2025']\"):  # 2025ë…„ ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ê¸°\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    full_url = \"https://www.vogue.co.kr\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # ì¤‘ë³µ ì œê±°\n",
    "        article_links.append(full_url)\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ì œëª© ì¶”ì¶œ\n",
    "            title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì²« ë²ˆì§¸ ì¤„ ì‚­ì œ, ë‘ ë²ˆì§¸ ì¤„ì„ ì—…ë¡œë“œ ë‚ ì§œë¡œ ì €ì¥\n",
    "            lines = content.split(\"\\n\")  # ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            if len(lines) > 1:\n",
    "                upload_date = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„ì´ ì—…ë¡œë“œ ë‚ ì§œ\n",
    "                content = \"\\n\".join(lines[2:]).strip()  # ì²«ì§¸ ì¤„ & ë‘˜ì§¸ ì¤„ ì œê±° í›„ ë³¸ë¬¸ ì¬êµ¬ì„±\n",
    "            else:\n",
    "                upload_date = \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                content = \"\"  # ë‚´ìš©ì´ ì—†ì„ ê²½ìš° ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬\n",
    "\n",
    "            # 4ï¸âƒ£ íŠ¹ì • íŒ¨í„´ ì´í›„ ëª¨ë“  ë‚´ìš© ì‚­ì œ\n",
    "            pattern = r\"(íŒ¨ì…˜ ë‰´ìŠ¤|íŒ¨ì…˜ íŠ¸ë Œë“œ|íŒ¨ì…˜ ì•„ì´í…œ|ì—¬í–‰|ì…€ëŸ¬ë¸Œë¦¬í‹° ìŠ¤íƒ€ì¼|ì›°ë‹ˆìŠ¤|ë·° í¬ì¸íŠ¸)\\n\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[ê°€-í£]+.*\"\n",
    "            content = re.sub(pattern, \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_vogue_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 15ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê¾¸ë¯¼ í‹° ì•ˆ ë‚˜ê²Œ ì•ˆê²½ ì“°ëŠ” ê°„ë‹¨í•œ ë°©ë²• | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.07\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„, ì™€ì´ë“œ íŒ¬ì¸ ëŠ” ì´ëŸ° ëŠë‚Œìœ¼ë¡œ ì…ìœ¼ì„¸ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê·¸ ì‹œì ˆ ë¸Œë¦¬íŠ¸ë‹ˆ ìŠ¤í”¼ì–´ìŠ¤ì˜ í™˜ìƒ? | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¯¸ì•¼ì˜¤ì™€ í•¨ê»˜í•œ ë”ë¸”ìœ  Vol.4 ì»¤ë²„ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í•˜ë£¨ì—ë„ ëª‡ ë²ˆì˜ ë³€ì‹ ! â€˜ë„ì´ì¹˜ íŒ¨ì…˜ìœ„í¬â€™ë¼ ë¶€ë¥¼ê²Œìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëŒì•„ì˜¨ ì»¨ì…‰ í€¸, ë ˆì´ë”” ê°€ê°€ì˜ ë‹¤í¬ ê¸€ë¨ ë£© | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì‚¬ì§„ê°€ ëª¨í•˜ë©”ë“œ ë¸Œë£¨ì´ì‚¬ì˜ ì‹œì„ ìœ¼ë¡œ ë³¸ í•˜ì´íŒ¨ì…˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„ ë¯¸ë‹ˆ ìŠ¤ì»¤íŠ¸ì—ëŠ”? íšŒìƒ‰ ì–‘ë§ VS ê²€ì • ìŠ¤íƒ€í‚¹ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.09\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‰´ ì‹œì¦Œ, ë‰´ íŒ¬ì¸ ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.09\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë©ˆì¶œ ì¤„ ëª¨ë¥´ëŠ” â€˜ì¹˜ë§ˆë°”ì§€â€™ ë°”ëŒ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.09\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì•„ì¹¨ì„ ì—¬ëŠ” ì‹œê³„ ë‹¤ë°œë“¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.09\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ ë¸”ë™ìœ¼ë¡œ ë©‹ ë‚´ëŠ” ë²•ì„ ì•Œë ¤ ì¤„ê²Œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.08\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S í‚¤ë£©ìœ¼ë¡œ ì—´ì–´ì –íŒ ë¸”ë¡ ë”” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.08\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì–´ë–¤ ìƒì˜ë¥¼ ì…ì–´ë„ ë©‹ìŠ¤ëŸ¬ì›Œì§€ëŠ” ì§„í•œ ë°ë‹˜ì˜ ë§› | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.08\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì»¤í”¼ë¶€í„° ë…¹ì°¨ê¹Œì§€, ë”°ìŠ¤í•œ ì—¬ìœ ë¡œì›€ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.08\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.11_wkorea_fashion_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Wì½”ë¦¬ì•„ íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.wkorea.com/fashion/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬ ë° ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸\n",
    "article_data = []\n",
    "\n",
    "# ê¸°ì‚¬ ëª©ë¡ í¬ë¡¤ë§ (`li` íƒœê·¸ ê¸°ì¤€ìœ¼ë¡œ íƒìƒ‰)\n",
    "for article in soup.select(\"li\"):\n",
    "    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸° (`a` íƒœê·¸ ë‚´ë¶€ì˜ href ì†ì„±)\n",
    "    a_tag = article.select_one(\"a[href^='https://www.wkorea.com/2025']\")\n",
    "    if not a_tag:\n",
    "        continue  # ê¸°ì‚¬ ë§í¬ê°€ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    article_url = a_tag[\"href\"]\n",
    "\n",
    "    # ê¸°ì‚¬ ì œëª© ì°¾ê¸° (`h3.s-tit` íƒœê·¸ ë‚´ë¶€)\n",
    "    title_tag = article.select_one(\"h3.s_tit\")\n",
    "    title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ URLê³¼ ì œëª©ì„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\"ì œëª©\": title, \"ê¸°ì‚¬ URL\": article_url})\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§ & ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ\n",
    "for article in article_data:\n",
    "    title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "             # 3ï¸âƒ£ ë³¸ë¬¸ ì²« ì¤„ & ë‘ ë²ˆì§¸ ì¤„ì—ì„œ ì—…ë¡œë“œ ë‚ ì§œ & ì—ë””í„° ì¶”ì¶œ\n",
    "            lines = content.split(\"\\n\")  # ë³¸ë¬¸ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            if len(lines) > 1:\n",
    "                # \"W Fashion YYYY.MM.DD\" í˜•ì‹ì—ì„œ \"W Fashion\" ì œê±° í›„ ë‚ ì§œë§Œ ì €ì¥\n",
    "                upload_date_match = re.match(r\"W Fashion\\s*(\\d{4}\\.\\d{2}\\.\\d{2})\", lines[0])\n",
    "                upload_date = upload_date_match.group(1) if upload_date_match else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                \n",
    "                editor = lines[1].strip()  # ë‘ ë²ˆì§¸ ì¤„ (ì—ë””í„°)\n",
    "            else:\n",
    "                upload_date = \"ë‚ ì§œ ì—†ìŒ\"\n",
    "                editor = \"ë¯¸ìƒ\"\n",
    "\n",
    "            # 4ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì²« ë‘ ì¤„(ì—…ë¡œë“œ ë‚ ì§œ & ì—ë””í„°) ì œê±°\n",
    "            content = \"\\n\".join(lines[2:]).strip() if len(lines) > 2 else \"\"\n",
    "\n",
    "            # 5ï¸âƒ£ ë³¸ë¬¸ ëë¶€ë¶„ì˜ \"W Fashion YYYY.MM.DD by ì´ë¦„\" íŒ¨í„´ ì œê±°\n",
    "            content = re.sub(r\"W\\s*Fashion\\n*\\d{4}\\.\\d{2}\\.\\d{2}by\\s*[ê°€-í£]+.*\", \"\", content, flags=re.DOTALL).strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 6ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_wkorea_fashion_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 30ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Micro Bags of the Season: ì‘ì€ ê°€ë°©ì´ ë§µë‹¤ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Flower Power: ëŸ°ì›¨ì´ ê½ƒë°­ì— ë†€ëŸ¬ ì™€ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Sweats: ì—ë””í„° ìŠ¤ì›» ìœ„ì‹œë¦¬ìŠ¤íŠ¸ 6ì¢… | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Fashion in Copenhagen: ìš°ë¦¬ê°€ ì½”íœí•˜ê² íŒ¨ì…˜ì— ëŒë¦¬ëŠ” ì´ìœ  | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Hitting the Slopes: PRADAë¶€í„° Chrome Heartsê¹Œì§€.ë¸Œëœë“œê°€ ìŠ¤í‚¤ ì•„ì´í…œì„ ë§Œë“ ë‹¤ë©´? | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Beyond the Logo: ê·¸ ë§ë˜ ë¡œê³ ëŠ” ë‹¤ ì–´ë””ë¡œ ê°”ì„ê¹Œ? | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Winter Fleece: ì´ê±´ ì‚¬ë‘ë©´ ì˜¤ë˜ ì…ê² ë‹¤ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Furry Club: í„¸ë¡œ ë¬´ì¥í•œ ê²¨ìš¸ ì¼ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Sabotage Street: ììœ ë¥¼ í–¥í•œ ì €í•­ì •ì‹  | ì—ë””í„°: Jente Store\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: RETRO ELEGANCE: ìš°ì•„í•¨ì„ í›”ì¹˜ëŠ” ì„¸ ê°€ì§€ ë°©ë²• | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Work Jacket: ë°”ëŒê³¼ í•¨ê»˜ ë¶ˆì–´ì˜¨ ì›Œí¬ ì¬í‚· ì—´í’ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Women's Boots: ì—ë””í„° ë¶€ì¸  ìœ„ì‹œë¦¬ìŠ¤íŠ¸ 6ì¢… | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Check Pattern: ì„¸ìƒì— ë‚˜ìœ ì²´í¬ëŠ” ì—†ë‹¤ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Camouflage: ê¼­ê¼­ ìˆ¨ì–´ë¼ ì¹´ë¬´í”Œë¼ì£¼ ë³´ì¼ë¼ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Running Clothes: ë” ë‹¬ë¦¬ê³  ì‹¶ê²Œ ë§Œë“¤ì–´ ì¤„ ëŸ¬ë‹ ì•„ì´í…œ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Boho Fashion: ììœ ë¥¼ ê¿ˆê¾¸ëŠ” ë³´í—¤ë¯¸ì•ˆ ìŠ¤íƒ€ì¼ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Grandpacore: ë©‹ìŸì´ í• ì•„ë²„ì§€ ë”°ë¼ì¡ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Socks: íŒ¨ì…˜ì˜ ì™„ì„±ì€ ì–‘ë§ì´ë‹¤ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 SS Menswear: ì‘ë‹µí•˜ë¼ 2025 SS ë§¨ì¦ˆì›¨ì–´ íŠ¸ë Œë“œ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Wear Black In Summer: ì• ì¦ì˜ ê´€ê³„, ì—¬ë¦„ê³¼ ë¸”ë™ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Fairycore: ìš”ì • ë‰´ì§„ìŠ¤, ì‡ ë§› ì—ìŠ¤íŒŒ ì–´ë””ì„œ ì˜¨ ê±¸ê¹Œ? | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Resort Wear: ìš°ë¦¬ ì—¬ë¦„ì—” ì´ë ‡ê²Œ ì…ê³  ê°™ì´ ë°”ë‹¤ ë³´ëŸ¬ ê°€ì | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Bella Hadid's Bag Collection: ë²¨ë¼ í•˜ë””ë“œì˜ ëºì–´ ë“¤ê³  ì‹¶ì€ ë°± ëª¨ìŒì§‘ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Girl Core: ê·¸ë…€ë“¤ì˜ íƒë‚˜ëŠ” ì•„ì´í…œ ì¶”ì ê¸° | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Geek Chic: ë‹¹ì‹ ì´ ê¸± ì‹œí¬ì— ëŒë¦¬ëŠ” ì´ìœ  | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: WHITE ON DENIM: ê·¹ë„ë¡œ ë°œë‹¬í•œ ê¾¸ì•ˆê¾¸, í™”ì´íŠ¸ì™€ ë°ë‹˜ | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Cowboy Core: ë¹Œë”© ìˆ²ì—ì„œ ì¹´ìš°ë³´ì´ ìŠ¤íƒ€ì¼ë¡œ ì‚´ì•„ë‚¨ê¸° | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Rebels in Fashion: í•˜ì§€ ë§ë¼ë©´ ë” í•˜ê³  ì‹¶ì–´! | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Peach Fuzz: 2024 ì˜¬í•´ì˜ ì»¬ëŸ¬ â€˜í”¼ì¹˜ í¼ì¦ˆâ€™ ì´ë ‡ê²Œ ì…ìœ¼ì‹œë©´ ë©ë‹ˆë‹¤ | ì—ë””í„°: ê¹€ë‚˜ì˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: Real Money: ëˆ ì˜ ë²„ëŠ” ì‚¬ëŒë“¤ì˜ ì˜· ì…ê¸° ê³µì‹ í•´ì²´í•˜ê¸° | ì—ë””í„°: ì£¼ë‹¨ë‹¨\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.11_jentestore_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Jente Store íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://jentestore.com/promotion?category=trend\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì§€ì •ëœ í˜•ì‹ì˜ ë§í¬ ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ë§í¬ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "article_links = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (index=x í˜•ì‹ ë§í¬)\n",
    "for a_tag in soup.select(\"a[href^='/promotion/event_view?no=']\"):\n",
    "    article_url = a_tag[\"href\"]\n",
    "    \n",
    "    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜\n",
    "    full_url = \"https://jentestore.com\" + article_url\n",
    "    \n",
    "    if full_url not in article_links:  # ì¤‘ë³µ ì œê±°\n",
    "        article_links.append(full_url)\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_links) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_links)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬!\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ì œëª©ê³¼ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for url in article_links:\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # ë³¸ë¬¸ì„ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬\n",
    "            lines = content.split(\"\\n\")\n",
    "\n",
    "            # 3ï¸âƒ£ ì œëª© ì„¤ì • (ì²« ë²ˆì§¸ ì¤„:ë‘ ë²ˆì§¸ ì¤„)\n",
    "            if len(lines) > 1:\n",
    "                title = f\"{lines[0]}: {lines[1]}\"\n",
    "            else:\n",
    "                title = \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "            # 4ï¸âƒ£ ë³¸ë¬¸ì—ì„œ ì„¸ ë²ˆì§¸ ì¤„ ê¹Œì§€ ì œê±°\n",
    "            content = \"\\n\".join(lines[3:]).strip() if len(lines) > 3 else \"\"\n",
    "\n",
    "             # 5ï¸âƒ£ \"ì—ë””í„°: í™ê¸¸ë™\" íŒ¨í„´ ì¶”ì¶œ & ë³¸ë¬¸ì—ì„œ ì‚­ì œ\n",
    "            editor_match = re.search(r\"ì—ë””í„°:\\s*([ê°€-í£]+)\", content)\n",
    "            editor = editor_match.group(1) if editor_match else \"Jente Store\"\n",
    "\n",
    "            # 6ï¸âƒ£ \"ì—ë””í„°: í™ê¸¸ë™\" ì´í›„ ë³¸ë¬¸ ì‚­ì œ\n",
    "            content = re.split(r\"ì—ë””í„°:\\s*[ê°€-í£]+\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ì—ë””í„°\": editor,  # ì—ë””í„° ì»¬ëŸ¼ ì¶”ê°€\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì œëª©\", \"ì—ë””í„°\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "csv_filename = f\"{scrape_date}_jentestore_trends.csv\"\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 20ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í´ë ŒëŠì™€ ì•„ì¼ëœë“œ ì—¬ì„±ë³µ ë””ìì´ë„ˆ ë¡œì´ì§„ í”¼ì–´ìŠ¤ì˜ ë§Œë‚¨ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.10 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: íœ´ê³  ë³´ìŠ¤, ì²« ë²ˆì§¸ ì•„íŠ¸ ë°”ì ¤ ì–´ì›Œë“œ í›„ì›! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.07 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì´ì†, ì²« ì£¼ì–¼ë¦¬ ì‘í’ˆ â€˜ì˜¤ë¥´ë„ˆ ì´ì–´ ì»¤í”„â€™ ì¶œì‹œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.07 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì•¤ì•„ë”ìŠ¤í† ë¦¬ì¦ˆ, ì„¸ê³„ ì—¬ì„±ì˜ ë‚  ê¸°ë… ì‹¤í¬ ìŠ¤ì¹´í”„ ì¶œì‹œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W íŒŒë¦¬ íŒ¨ì…˜ìœ„í¬ë¥¼ ë¹›ë‚¸ ìŠ¤íŠ¸ë¦¬íŠ¸ ìŠ¤íƒ€ì¼ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë””ì˜¬, ì„¸ê³„ ì—¬ì„±ì˜ ë‚  ê¸°ë… ë‹¤íë©˜í„°ë¦¬ ê³µê°œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì•Œë¼ì´ì•„ì™€ ë®ˆê¸€ëŸ¬, 80ë…„ëŒ€ íŒ¨ì…˜ì„ ì •ì˜í•œ ë‘ ê±°ì¥ì˜ ìš°ì •ê³¼ ìœ ì‚° | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¬´ë¼ì¹´ë¯¸ ë‹¤ì¹´ì‹œ, MLB ë„ì¿„ ì‹œë¦¬ì¦ˆ ìœ„í•œ í•œì •íŒ ì»¬ë ‰ì…˜ ê³µê°œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¼ë¶€ë¶€, ê¸€ë¡œë²Œ ì…€ëŸ½ë“¤ì´ ì£¼ëª©í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ì…˜ ì•„ì´í…œ? | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.06 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë°€ë€íŒ¨ì…˜ìœ„í¬ì—ì„œ í¬ì°©í•œ ë² ìŠ¤íŠ¸ ìŠ¤íŠ¸ë¦¬íŠ¸ íŒ¨ì…˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë„í”„ ë¡œë Œ, MLB ë„ì¿„ ì‹œë¦¬ì¦ˆ ê¸°ë… ìº¡ìŠ ì»¬ë ‰ì…˜ ì¶œì‹œ! | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¦¬í…Œì¼ ì „ë¬¸ê°€ë“¤ì´ ì „í•˜ëŠ” ë°€ë€ íŒ¨ì…˜ìœ„í¬ íŠ¸ë Œë“œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W ë°€ë€ íŒ¨ì…˜ìœ„í¬, ì£¼ëª©í•  ë§Œí•œ ë°ë·” ë¸Œëœë“œ 3 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.05 | ì—ë””í„°: ë°•ë‹¤ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: íœë””, 100ë…„ì˜ í—¤ë¦¬í‹°ì§€ë¥¼ ë‹´ì€ ëŸ°ì›¨ì´ ì‡¼ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë¹„ìš˜ì„¸ì™€ ë¦¬ë°”ì´ìŠ¤ì˜ ë‘ ë²ˆì§¸ ë§Œë‚¨ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì œ97íšŒ ì˜¤ìŠ¤ì¹´ ì‹œìƒì‹ì„ ë¹›ë‚¸ ë² ìŠ¤íŠ¸ ë“œë ˆì„œë“¤ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.03.04 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëŸ­ì…”ë¦¬ ë¸Œëœë“œë“¤ì´ ì„ ë³´ì¸ 2025 S/S ìº í˜ì¸ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: íŒ¨ì…˜ ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´í„°ì—ì„œ ì¶”ìƒ í™”ê°€ë¡œ, íƒ€ëƒ ë§ì˜ ë³€ì‹  | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.28 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ê¸€ë¡œë²Œ í†± íŒ¨ì…˜ ë””ìì´ë„ˆê°€ ì„ ì •í•œ 2025 ë§¨ì¦ˆ íŒ¨ì…˜ ë² ìŠ¤íŠ¸ ì•„ì´í…œ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27 | ì—ë””í„°: ê¹€ë¯¼ì •\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í—¬ë ˆë‚˜ í¬ë¦¬ìŠ¤í…ìŠ¨, ë³´ì»¨ì…‰ ê¸€ë¡œë²Œ ì•„íŠ¸ ë””ë ‰í„°ë¡œ í•©ë¥˜ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.27 | ì—ë””í„°: ìµœì£¼ì—°\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.11_wwdkorea_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## WWD Korea íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.wwdkorea.com/news/articleList.html?sc_section_code=S1N3&view_type=sm\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª©, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "article_data = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ë‰´ìŠ¤ ê¸°ì‚¬ ë§í¬)\n",
    "for article in soup.select(\"li div.views\"):\n",
    "    # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°\n",
    "    a_tag = article.select_one(\"h4.titles a[href^='/news/articleView.html?idxno=']\")\n",
    "    if not a_tag:\n",
    "        continue  # ë§í¬ ì—†ìœ¼ë©´ ë¬´ì‹œ\n",
    "\n",
    "    article_url = \"https://www.wwdkorea.com\" + a_tag[\"href\"]\n",
    "\n",
    "    # ê¸°ì‚¬ ì œëª© ì°¾ê¸° (h4.titles ë‚´ë¶€ì˜ a íƒœê·¸)\n",
    "    title = a_tag.get_text(strip=True) if a_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "    # ì—…ë¡œë“œ ë‚ ì§œ ì°¾ê¸°\n",
    "    date_tag = article.select_one(\"em.info.dated\")\n",
    "    upload_date = date_tag.get_text(strip=True) if date_tag else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "\n",
    "    # ì—ë””í„° ì°¾ê¸°\n",
    "    editor_tag = article.select_one(\"em.info.name\")\n",
    "    editor = editor_tag.get_text(strip=True).replace(\" ì—ë””í„°\", \"\") if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "    # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\n",
    "        \"ì œëª©\": title,\n",
    "        \"ê¸°ì‚¬ URL\": article_url,\n",
    "        \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "        \"ì—ë””í„°\": editor\n",
    "    })\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸ í¬ë¡¤ë§\n",
    "for article in article_data:\n",
    "    title, url, upload_date, editor = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"], article[\"ì—…ë¡œë“œ ë‚ ì§œ\"], article[\"ì—ë””í„°\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # 3ï¸âƒ£ \"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>ê°€\" ë˜ëŠ” \"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>ì˜\" ì´í›„ ë‚´ìš© ì‚­ì œ\n",
    "            content = re.split(r\"<ë”ë¸”ìœ ë”ë¸”ìœ ë””ì½”ë¦¬ì•„>(ê°€|ì˜)\", content, maxsplit=1)[0].strip()\n",
    "\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: wwdkorea_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_wwdkorea_trends.csv\"\n",
    "\n",
    "# 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ì´ 10ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #5 | Neo-Bourgeois | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ì´ë‹¤ì€\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #2 | Belly, On The Sly | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ê¹€ì§€ìˆ˜\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 S/S íŒ¨ì…˜ íŠ¸ë Œë“œ #1 | Layered Elegance | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.25 | ì—ë””í„°: ì •í‰í™”\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì¼„ë‹¬ ì œë„ˆë„ ê½‚íŒ í—ŒíŒ…ìº¡ ìŠ¤íƒ€ì¼ë§ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.21 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ë‚©ì‘í• ìˆ˜ë¡ ìŠ¤íƒ€ì¼ë¦¬ì‹œí•œ ìŠ¤ë‹ˆì»¤ì¦ˆ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.19 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì œë‹ˆë„ ë°˜í•œ í¬ì¼“ ë°± | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.12 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ëª©ì´ í—ˆì „í•  ë•Œ, ìŠ¤í‚¤ë‹ˆ ìŠ¤ì¹´í”„ | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.09 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: ì˜¬ë´„, ìŠˆíŠ¸ì˜ ì‹œëŒ€ê°€ ë„ë˜í•  ì˜ˆì • | ì—…ë¡œë“œ ë‚ ì§œ: 2025.02.06 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: 2025 F/W íŒŒë¦¬ ë§¨ì¦ˆ íŒ¨ì…˜ìœ„í¬ì— ì°¸ì„í•œ ì…€ëŸ½ì€? Part 2 | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.31 | ì—ë””í„°: ê¹€ ì›(í”„ë¦¬ëœì„œ)\n",
      "âœ… í¬ë¡¤ë§ ì„±ê³µ: í•´ì¹˜ì§€ ì•Šì•„ìš” | ì—…ë¡œë“œ ë‚ ì§œ: 2025.01.31 | ì—ë””í„°: ê¹€ì§€ìˆ˜\n",
      "ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: 2025.03.11_marieclaire_trends.csv\n"
     ]
    }
   ],
   "source": [
    "## Marie Claire íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ##\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "main_url = \"https://www.marieclairekorea.com/category/fashion/fashion_trend/\"\n",
    "\n",
    "# HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "response = requests.get(main_url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "article_data = []\n",
    "\n",
    "# ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ê¸°ì‚¬ ë§í¬ & ì œëª©)\n",
    "for article in soup.select(\"h2.entry-title a\"):\n",
    "    article_url = article[\"href\"]\n",
    "    title = article.get_text(strip=True)\n",
    "\n",
    "    # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "    article_data.append({\n",
    "        \"ì œëª©\": title,\n",
    "        \"ê¸°ì‚¬ URL\": article_url\n",
    "    })\n",
    "\n",
    "    # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "    if len(article_data) >= 30:\n",
    "        break\n",
    "\n",
    "print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 30ê°œ)\")\n",
    "\n",
    "# í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "data = []\n",
    "\n",
    "# í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "scrape_date = datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° í¬ë¡¤ë§\n",
    "for article in article_data:\n",
    "    title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "            # ğŸ“Œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (`span.updated.rich-snippet-hidden`)\n",
    "            date_tag = soup.select_one(\"span.updated.rich-snippet-hidden\")\n",
    "            upload_date = date_tag.get_text(strip=True)[:10] if date_tag else \"ë‚ ì§œ ì—†ìŒ\" \n",
    "\n",
    "            # ë‚ ì§œ ë³€í™˜ (YYYY-MM-DD â†’ YYYY.MM.DD)\n",
    "            upload_date = upload_date.replace(\"-\", \".\")\n",
    "\n",
    "            # ğŸ“Œ ì—ë””í„° ì¶”ì¶œ (`span.fn a`)\n",
    "            editor_tag = soup.select_one(\"span.fn a\")\n",
    "            editor = editor_tag.get_text(strip=True) if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "            # ğŸ“Œ ìš”ì•½ ë‚´ìš© ì¶”ì¶œ (`div.mck_post_excerpt`)\n",
    "            excerpt_tag = soup.select_one(\"div.mck_post_excerpt\")\n",
    "            excerpt = excerpt_tag.get_text(strip=True) if excerpt_tag else \"\"\n",
    "\n",
    "            # ğŸ“Œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (`div.post-content p`)\n",
    "            paragraphs = soup.select(\"div.post-content p\")\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs])\n",
    "\n",
    "            # ğŸ“Œ ìµœì¢… ë³¸ë¬¸ = ìš”ì•½ + ë³¸ë¬¸\n",
    "            full_content = f\"{excerpt}\\n{content}\".strip()\n",
    "\n",
    "            # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "            data.append({\n",
    "                \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "                \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "                \"ì—ë””í„°\": editor,\n",
    "                \"ì œëª©\": title,\n",
    "                \"ë³¸ë¬¸\": full_content,\n",
    "                \"ê¸°ì‚¬ URL\": url\n",
    "            })\n",
    "            print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: marieclaire_trends_2025-03-06.csv)\n",
    "csv_filename = f\"{scrape_date}_marieclaire_trends.csv\"\n",
    "\n",
    "# 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Cosmopolitan íŒ¨ì…˜ íŠ¸ë Œë“œ í˜ì´ì§€ í¬ë¡¤ë§  ## --> ì•„ì§ ë³´ë¥˜\n",
    "\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "\n",
    "# # í¬ë¡¤ë§í•  ë©”ì¸ í˜ì´ì§€ URL\n",
    "# main_url = \"https://www.cosmopolitan.co.kr/fashion/trends/\"\n",
    "\n",
    "# # HTTP ìš”ì²­ í—¤ë” ì„¤ì • (ë´‡ ì°¨ë‹¨ ë°©ì§€)\n",
    "# headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# # 1ï¸âƒ£ ë©”ì¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ë§í¬, ì œëª© ê°€ì ¸ì˜¤ê¸°\n",
    "# response = requests.get(main_url, headers=headers)\n",
    "# soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "# # ê¸°ì‚¬ ì •ë³´ ì €ì¥ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 30ê°œ í¬ë¡¤ë§)\n",
    "# article_data = []\n",
    "\n",
    "# # ì§€ì •ëœ í˜•ì‹ì˜ <a> íƒœê·¸ ì°¾ê¸° (ê¸°ì‚¬ ë§í¬ & ì œëª©)\n",
    "# for article in soup.select(\"div.txtbox\"):\n",
    "#     # ê¸°ì‚¬ ì œëª© ì°¾ê¸°\n",
    "#     title_tag = article.select_one(\"p.tit\")\n",
    "#     title = title_tag.get_text(strip=True) if title_tag else \"ì œëª© ì—†ìŒ\"\n",
    "\n",
    "#     # ê¸°ì‚¬ ë§í¬ ì°¾ê¸°\n",
    "#     a_tag = article.find_parent(\"a\", href=True)\n",
    "#     article_url = \"https://www.cosmopolitan.co.kr\" + a_tag[\"href\"] if a_tag else \"ë§í¬ ì—†ìŒ\"\n",
    "\n",
    "#     # ê¸°ì‚¬ ì •ë³´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€\n",
    "#     article_data.append({\n",
    "#         \"ì œëª©\": title,\n",
    "#         \"ê¸°ì‚¬ URL\": article_url\n",
    "#     })\n",
    "\n",
    "#     # 30ê°œê¹Œì§€ë§Œ í¬ë¡¤ë§\n",
    "#     if len(article_data) >= 24:\n",
    "#         break\n",
    "\n",
    "# print(f\"ğŸ”— ì´ {len(article_data)}ê°œì˜ ê¸°ì‚¬ ë§í¬ ë°œê²¬! (ìµœëŒ€ 24ê°œ)\")\n",
    "\n",
    "# # í¬ë¡¤ë§ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "# data = []\n",
    "\n",
    "# # í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ë‚ ì§œ)\n",
    "# scrape_date = datetime.today().strftime(\"%Y.%m.%d\")  # YYYY.MM.DD í˜•ì‹\n",
    "\n",
    "# # 2ï¸âƒ£ ê° ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ ë³¸ë¬¸, ì—…ë¡œë“œ ë‚ ì§œ, ì—ë””í„° í¬ë¡¤ë§\n",
    "# for article in article_data:\n",
    "#     title, url = article[\"ì œëª©\"], article[\"ê¸°ì‚¬ URL\"]\n",
    "    \n",
    "#     try:\n",
    "#         response = requests.get(url, headers=headers)\n",
    "        \n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.text, \"lxml\")\n",
    "\n",
    "#             # ğŸ“Œ ì—…ë¡œë“œ ë‚ ì§œ ì¶”ì¶œ (`span.time`)\n",
    "#             date_tag = soup.select_one(\"span.time\")\n",
    "#             upload_date = date_tag.get_text(strip=True) if date_tag else \"ë‚ ì§œ ì—†ìŒ\"\n",
    "\n",
    "#             # ğŸ“Œ ì—ë””í„° ì¶”ì¶œ (`a[href^='/editorlist/detail/']`)\n",
    "#             editor_tag = soup.select_one(\"a[href^='/editorlist/detail/']\")\n",
    "#             editor = editor_tag.get_text(strip=True) if editor_tag else \"ë¯¸ìƒ\"\n",
    "\n",
    "#             # ğŸ“Œ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ (`p`, `div.ab_photo`) - íŠ¹ì • í˜•ì‹ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ & íŠ¹ì • íƒœê·¸ ì´í›„ í¬ë¡¤ë§ ì¤‘ì§€\n",
    "#             content_list = []\n",
    "#             tag_atc_wrap = soup.select_one(\"div.tag_atc_wrap\")\n",
    "#             paragraphs = soup.select(\"p.ab_emphasis_content, p:not([class]), div.ab_photo\")\n",
    "#             for p in paragraphs:\n",
    "#                 if tag_atc_wrap and p in tag_atc_wrap.find_all(\"p\"):\n",
    "#                     break\n",
    "#                 text = p.get_text(strip=True)\n",
    "#                 content_list.append(text)\n",
    "#             content = \"\\n\".join(content_list)\n",
    "\n",
    "\n",
    "#             # ë¦¬ìŠ¤íŠ¸ì— ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ë°°ì¹˜)\n",
    "#             data.append({\n",
    "#                 \"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\": scrape_date,\n",
    "#                 \"ì—…ë¡œë“œ ë‚ ì§œ\": upload_date,\n",
    "#                 \"ì—ë””í„°\": editor,\n",
    "#                 \"ì œëª©\": title,\n",
    "#                 \"ë³¸ë¬¸\": content,\n",
    "#                 \"ê¸°ì‚¬ URL\": url\n",
    "#             })\n",
    "#             print(f\"âœ… í¬ë¡¤ë§ ì„±ê³µ: {title} | ì—…ë¡œë“œ ë‚ ì§œ: {upload_date} | ì—ë””í„°: {editor}\")\n",
    "\n",
    "#         else:\n",
    "#             print(f\"â›” ìš”ì²­ ì‹¤íŒ¨ ({response.status_code}): {url}\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"âš ï¸ ì˜¤ë¥˜ ë°œìƒ: {url} - {str(e)}\")\n",
    "\n",
    "# # 3ï¸âƒ£ CSV íŒŒì¼ëª…ì— ìŠ¤í¬ë˜í•‘ ë‚ ì§œ í¬í•¨ (ì˜ˆ: cosmopolitan_trends_2025.03.06.csv)\n",
    "# csv_filename = f\"{scrape_date}_cosmopolitan_trends2.csv\"\n",
    "\n",
    "# # 4ï¸âƒ£ CSV íŒŒì¼ë¡œ ì €ì¥ (URLì„ ë§ˆì§€ë§‰ ì»¬ëŸ¼ìœ¼ë¡œ ì •ë ¬)\n",
    "# df = pd.DataFrame(data, columns=[\"ìŠ¤í¬ë˜í•‘ ë‚ ì§œ\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì—ë””í„°\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"])\n",
    "# df.to_csv(csv_filename, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "# print(f\"ğŸ“‚ ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ! CSV íŒŒì¼ ì €ì¥ë¨: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File Merge ##\n",
    "\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# ğŸ“Œ í˜„ì¬ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°\n",
    "date = datetime.datetime.today().strftime(\"%Y.%m.%d\")\n",
    "\n",
    "# ğŸ“Œ ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (íŒŒì¼ëª…ì— ë§¤ê±°ì§„ëª… í¬í•¨)\n",
    "file_paths = {\n",
    "    # f\"{date}_jentestore_trends.csv\": \"Jentestore\",\n",
    "    f\"{date}_marieclaire_trends.csv\": \"Marie Claire\",\n",
    "    f\"{date}_vogue_fashion_trends.csv\": \"Vogue\",\n",
    "    f\"{date}_wkorea_fashion_trends.csv\": \"W Korea\",\n",
    "    f\"{date}_wwdkorea_trends.csv\": \"WWD Korea\"\n",
    "}\n",
    "\n",
    "# ğŸ“Œ ëª¨ë“  ë°ì´í„°ë¥¼ ë³‘í•©í•  ë¦¬ìŠ¤íŠ¸\n",
    "dataframes = []\n",
    "\n",
    "# ğŸ“Œ CSV íŒŒì¼ ì½ê¸° ë° ë³‘í•© (ë§¤ê±°ì§„ ëª… ì¶”ê°€)\n",
    "for file_path, magazine_name in file_paths.items():\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, encoding=\"utf-8-sig\")\n",
    "        df[\"ë§¤ê±°ì§„ ëª…\"] = magazine_name  # ë§¤ê±°ì§„ ëª… ì»¬ëŸ¼ ì¶”ê°€\n",
    "        dataframes.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ íŒŒì¼ ì—†ìŒ: {file_path}\")\n",
    "\n",
    "# ğŸ“Œ ë°ì´í„° ë³‘í•©\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# ğŸ“Œ í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (ë§¤ê±°ì§„ ëª… í¬í•¨)\n",
    "columns_needed = [\"ë§¤ê±°ì§„ ëª…\", \"ì—…ë¡œë“œ ë‚ ì§œ\", \"ì œëª©\", \"ë³¸ë¬¸\", \"ê¸°ì‚¬ URL\"]\n",
    "existing_columns = [col for col in columns_needed if col in merged_df.columns]\n",
    "cleaned_df = merged_df[existing_columns].dropna().reset_index(drop=True)\n",
    "\n",
    "# ğŸ“Œ ë³‘í•©ëœ ë°ì´í„° ì €ì¥\n",
    "merged_file_path = f\"{date}_merged_fashion_trends.csv\"\n",
    "cleaned_df.to_csv(merged_file_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"âœ… ë³‘í•© ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {merged_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpmg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
